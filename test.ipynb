{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.12.0 transformers==4.21.0 nltk==3.7 numpy==1.21.2 scikit-learn==1.0.2 regex==2022.3.15 stanfordcorenlp==3.9.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tải dữ liệu cần thiết cho NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class MDERank:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", pooling=\"max\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def compute_embedding(self, text):\n",
    "        # Chuẩn hóa đầu vào và lấy output từ mô hình BERT\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Lấy hidden_states: [batch_size, sequence_length, hidden_size]\n",
    "        hidden_states = outputs.last_hidden_state[0]  # (seq_len, hidden_size)\n",
    "        if self.pooling == \"max\":\n",
    "            embedding, _ = torch.max(hidden_states, dim=0)\n",
    "        elif self.pooling == \"avg\":\n",
    "            embedding = torch.mean(hidden_states, dim=0)\n",
    "        else:\n",
    "            embedding = torch.mean(hidden_states, dim=0)\n",
    "        return embedding.numpy()\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        \"\"\"\n",
    "        Sử dụng NLTK để tách từ, gán nhãn POS và trích xuất các cụm từ ứng viên\n",
    "        theo pattern: liên tục các từ có tag bắt đầu bằng JJ (tính từ) hoặc NN (danh từ).\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        candidates = []\n",
    "        candidate = []\n",
    "        for word, tag in tagged:\n",
    "            if tag.startswith(\"JJ\") or tag.startswith(\"NN\"):\n",
    "                candidate.append(word)\n",
    "            else:\n",
    "                if candidate:\n",
    "                    phrase = \" \".join(candidate)\n",
    "                    candidates.append(phrase)\n",
    "                    candidate = []\n",
    "        if candidate:\n",
    "            phrase = \" \".join(candidate)\n",
    "            candidates.append(phrase)\n",
    "        # Loại bỏ các cụm từ trùng lặp và có độ dài ít nhất 1 từ\n",
    "        candidates = list(set([c for c in candidates if len(c.split()) >= 1]))\n",
    "        return candidates\n",
    "\n",
    "    def mask_text(self, text, candidate):\n",
    "        \"\"\"\n",
    "        Thay thế các xuất hiện của candidate trong text bằng [MASK] với số lượng token tương ứng.\n",
    "        \"\"\"\n",
    "        candidate_tokens = candidate.split()\n",
    "        mask_token = \" \".join([\"[MASK]\"] * len(candidate_tokens))\n",
    "        # Sử dụng regex để thay thế, không phân biệt hoa thường\n",
    "        pattern = re.compile(re.escape(candidate), re.IGNORECASE)\n",
    "        masked_text = pattern.sub(mask_token, text)\n",
    "        return masked_text\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n",
    "\n",
    "    def rank_keyphrases(self, text):\n",
    "        \"\"\"\n",
    "        Tính toán embedding của văn bản gốc và đối với mỗi ứng viên, tính embedding của văn bản đã mask.\n",
    "        Sau đó, tính cosine similarity giữa hai embedding này. Ứng viên có similarity thấp hơn (nghĩa là mất thông tin lớn)\n",
    "        được xem là quan trọng hơn.\n",
    "        \"\"\"\n",
    "        original_embedding = self.compute_embedding(text)\n",
    "        candidates = self.extract_candidates(text)\n",
    "        scores = {}\n",
    "        for candidate in candidates:\n",
    "            masked_text = self.mask_text(text, candidate)\n",
    "            masked_embedding = self.compute_embedding(masked_text)\n",
    "            sim = self.cosine_similarity(original_embedding, masked_embedding)\n",
    "            scores[candidate] = sim\n",
    "        # Sắp xếp các ứng viên theo thứ tự tăng dần của similarity\n",
    "        ranked = sorted(scores.items(), key=lambda x: x[1])\n",
    "        return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_keyphrases(text, top_k=10):\n",
    "    \"\"\"\n",
    "    Hàm bọc để trích xuất keyphrase từ văn bản.\n",
    "    Trả về danh sách top_k keyphrase có score thấp nhất (nghĩa là quan trọng nhất).\n",
    "    \"\"\"\n",
    "    mde = MDERank()\n",
    "    ranked = mde.rank_keyphrases(text.lower())\n",
    "    top_candidates = [phrase for phrase, score in ranked[:top_k]]\n",
    "    return top_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_labels(labels):\n",
    "    clean_labels = {}\n",
    "    for id in labels:\n",
    "        label = labels[id]\n",
    "        clean_label = []\n",
    "        for kp in label:\n",
    "            if kp.find(\";\") != -1:\n",
    "                left, right = kp.split(\";\")\n",
    "                clean_label.append(left)\n",
    "                clean_label.append(right)\n",
    "            else:\n",
    "                clean_label.append(kp)\n",
    "        clean_labels[id] = clean_label        \n",
    "    return clean_labels\n",
    "\n",
    "\n",
    "def get_long_data(file_path=\"data/nus/nus_test.json\"):\n",
    "    \"\"\" Load file.jsonl .\"\"\"\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "        json_text = f.readlines()\n",
    "        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n",
    "            try:\n",
    "                jsonl = json.loads(line)\n",
    "                keywords = jsonl['keywords'].lower().split(\";\")\n",
    "                abstract = jsonl['abstract']\n",
    "                fulltxt = jsonl['fulltext']\n",
    "                doc = ' '.join([abstract, fulltxt])\n",
    "                doc = re.sub('\\. ', ' . ', doc)\n",
    "                doc = re.sub(', ', ' , ', doc)\n",
    "                doc = doc.replace('\\n', ' ')\n",
    "                data[jsonl['name']] = doc\n",
    "                labels[jsonl['name']] = keywords\n",
    "            except:\n",
    "                raise ValueError\n",
    "    labels = clean_labels(labels)\n",
    "    return data,labels\n",
    "\n",
    "\n",
    "def get_duc2001_data(file_path=\"data/DUC2001\"):\n",
    "    pattern = re.compile(r'<TEXT>(.*?)</TEXT>', re.S)\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    for dirname, dirnames, filenames in os.walk(file_path):\n",
    "        for fname in filenames:\n",
    "            if (fname == \"annotations.txt\"):\n",
    "                # left, right = fname.split('.')\n",
    "                infile = os.path.join(dirname, fname)\n",
    "                f = open(infile,'rb')\n",
    "                text = f.read().decode('utf8')\n",
    "                lines = text.splitlines()\n",
    "                for line in lines:\n",
    "                    left, right = line.split(\"@\")\n",
    "                    d = right.split(\";\")[:-1]\n",
    "                    l = left\n",
    "                    labels[l] = d\n",
    "                f.close()\n",
    "            else:\n",
    "                infile = os.path.join(dirname, fname)\n",
    "                f = open(infile,'rb')\n",
    "                text = f.read().decode('utf8')\n",
    "                text = re.findall(pattern, text)[0]\n",
    "                data[fname] = text\n",
    "    labels = clean_labels(labels)\n",
    "    return data,labels\n",
    "\n",
    "def get_inspec_data(file_path=\"data/Inspec\"):\n",
    "\n",
    "    data={}\n",
    "    labels={}\n",
    "    for dirname, dirnames, filenames in os.walk(file_path):\n",
    "        for fname in filenames:\n",
    "            left, right = fname.split('.')\n",
    "            if (right == \"abstr\"):\n",
    "                infile = os.path.join(dirname, fname)\n",
    "                f=open(infile)\n",
    "                text=f.read()\n",
    "                text = text.replace(\"%\", '')\n",
    "                data[left]=text\n",
    "            if (right == \"uncontr\"):\n",
    "                infile = os.path.join(dirname, fname)\n",
    "                f=open(infile)\n",
    "                text=f.read()\n",
    "                text = text.replace(\"\\n\\t\", ' ')\n",
    "                text=text.replace(\"\\n\",' ')\n",
    "                label=text.split(\"; \")\n",
    "                labels[left]=label\n",
    "    labels = clean_labels(labels)\n",
    "    return data,labels\n",
    "\n",
    "def get_semeval2017_data(data_path=\"data/SemEval2017/docsutf8\",labels_path=\"data/SemEval2017/keys\"):\n",
    "\n",
    "    data={}\n",
    "    labels={}\n",
    "    for dirname, dirnames, filenames in os.walk(data_path):\n",
    "        for fname in filenames:\n",
    "            left, right = fname.split('.')\n",
    "            infile = os.path.join(dirname, fname)\n",
    "            # f = open(infile, 'rb')\n",
    "            # text = f.read().decode('utf8')\n",
    "            with codecs.open(infile, \"r\", \"utf-8\") as fi:\n",
    "                text = fi.read()\n",
    "                text = text.replace(\"%\", '')\n",
    "            data[left] = text.lower()\n",
    "            # f.close()\n",
    "    for dirname, dirnames, filenames in os.walk(labels_path):\n",
    "        for fname in filenames:\n",
    "            left, right = fname.split('.')\n",
    "            infile = os.path.join(dirname, fname)\n",
    "            f = open(infile, 'rb')\n",
    "            text = f.read().decode('utf8')\n",
    "            text = text.strip()\n",
    "            ls=text.splitlines()\n",
    "            labels[left] = ls\n",
    "            f.close()\n",
    "    labels = clean_labels(labels)\n",
    "    return data,labels\n",
    "\n",
    "def get_short_data(file_path=\"data/krapivin/kravipin_test.json\"):\n",
    "    \"\"\" Load file.jsonl .\"\"\"\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "        json_text = f.readlines()\n",
    "        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n",
    "            try:\n",
    "                jsonl = json.loads(line)\n",
    "                keywords = jsonl['keywords'].lower().split(\";\")\n",
    "                abstract = jsonl['abstract']\n",
    "                doc =abstract\n",
    "                doc = re.sub('\\. ', ' . ', doc)\n",
    "                doc = re.sub(', ', ' , ', doc)\n",
    "                doc = doc.replace('\\n', ' ')\n",
    "                doc = doc.replace('\\t', ' ')\n",
    "                data[i] = doc\n",
    "                labels[i] = keywords\n",
    "            except:\n",
    "                raise ValueError\n",
    "    labels = clean_labels(labels)\n",
    "    return data,labels\n",
    "\n",
    "def get_krapivin_data(file_path=\"data/krapivin/krapivin_test.json\"):\n",
    "    return get_short_data(file_path)\n",
    "\n",
    "def get_nus_data(file_path=\"data/nus/nus_test.json\"):\n",
    "    return get_long_data(file_path)\n",
    "\n",
    "def get_semeval2010_data(file_path=\"data/SemEval2010/semeval_test.json\"):\n",
    "    return get_short_data(file_path)\n",
    "\n",
    "def get_dataset_data(dataset_name):\n",
    "    if dataset_name == \"duc2001\":\n",
    "        return get_duc2001_data()\n",
    "    elif dataset_name == \"inspec\":\n",
    "        return get_inspec_data()\n",
    "    elif dataset_name == \"krapivin\":\n",
    "        return get_krapivin_data()\n",
    "    elif dataset_name == \"nus\":\n",
    "        return get_nus_data()\n",
    "    elif dataset_name == \"semeval2010\":\n",
    "        return get_semeval2010_data()\n",
    "    elif dataset_name == \"sameval2017\":\n",
    "        return get_semeval2017_data()\n",
    "    \n",
    "def calculate_f1(predicted, ground_truth, k)->float:\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1@K.\n",
    "    \n",
    "    Parameters:\n",
    "      predicted (list): List of predicted keyphrases.\n",
    "      ground_truth (list): List of ground truth keyphrases.\n",
    "      k (int): The cutoff for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    predicted_top_k = predicted[:k]\n",
    "    common = set(predicted_top_k) & set(ground_truth)\n",
    "    precision = len(common) * 1.0 / k if k > 0 else 0\n",
    "    recall = len(common) * 1.0 / len(ground_truth) if ground_truth else 0\n",
    "    f1 = 0\n",
    "    if precision + recall > 0:\n",
    "       f1 = 200.0 * precision * recall / (precision + recall)\n",
    "    # return precision, recall, f1\n",
    "    return f1\n",
    "\n",
    "\n",
    "def print_to_json(data_name, k, score):\n",
    "    \"\"\"\n",
    "    Print the evaluation results to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "      data_name (str): The name of the dataset.\n",
    "      k (int): The cutoff for evaluation.\n",
    "      score (list): The list of evaluation results.\n",
    "    \"\"\"\n",
    "    average_score = sum(score) / len(score) if score else 0\n",
    "    result = {\n",
    "        \"dataset\": data_name,\n",
    "        \"top_k\": k,\n",
    "        \"average_score\": average_score,}\n",
    "    with open(f\"results/{data_name}_{k}.json\", \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset = ['duc2001', 'inspec', 'krapivin', 'nus', 'semeval2010', 'sameval2017']\n",
    "    dataset = ['krapivin']\n",
    "    \n",
    "\n",
    "    for data_name in dataset:\n",
    "        data,labels = get_dataset_data(data_name) \n",
    "        score5 = []\n",
    "        score10 = []\n",
    "        score15 = []\n",
    "        for id in data:\n",
    "            keyphrases = extract_keyphrases(data[id], top_k=15)\n",
    "            \n",
    "            score5.append(calculate_f1(keyphrases, labels[id], 5))\n",
    "            score10.append(calculate_f1(keyphrases, labels[id], 10))\n",
    "            score15.append(calculate_f1(keyphrases, labels[id], 15))\n",
    "        print_to_json(data_name, 5, score5)\n",
    "        print_to_json(data_name, 10, score10)\n",
    "        print_to_json(data_name, 15, score15)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
