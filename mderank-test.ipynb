{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11114107,"sourceType":"datasetVersion","datasetId":6929593}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary packages (run these cells if not already installed)\n!pip install stanfordcorenlp==3.9.1.1\n!pip install torchvision\n\n# Imports\nimport re\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel, T5Tokenizer\nimport nltk\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset\nfrom stanfordcorenlp import StanfordCoreNLP\nfrom tqdm import tqdm\nimport json\nimport os\nimport sys\nimport codecs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:38.873822Z","iopub.execute_input":"2025-03-24T04:39:38.874144Z","iopub.status.idle":"2025-03-24T04:39:55.287044Z","shell.execute_reply.started":"2025-03-24T04:39:38.874120Z","shell.execute_reply":"2025-03-24T04:39:55.286316Z"}},"outputs":[{"name":"stdout","text":"Collecting stanfordcorenlp==3.9.1.1\n  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp==3.9.1.1) (5.9.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp==3.9.1.1) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (2025.1.31)\nDownloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\nInstalling collected packages: stanfordcorenlp\nSuccessfully installed stanfordcorenlp-3.9.1.1\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Check working directory (optional)\n!pwd\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nclass MDERank:\n    def __init__(self, model_name=\"bert-base-uncased\", pooling=\"max\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.pooling = pooling\n\n    def compute_embedding(self, text):\n        # Tokenize and get model output\n        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Get last hidden states: [batch_size, sequence_length, hidden_size]\n        hidden_states = outputs.last_hidden_state[0]  # (seq_len, hidden_size)\n        if self.pooling == \"max\":\n            embedding, _ = torch.max(hidden_states, dim=0)\n        elif self.pooling == \"avg\":\n            embedding = torch.mean(hidden_states, dim=0)\n        else:\n            embedding = torch.mean(hidden_states, dim=0)\n        return embedding.numpy()\n\n    def extract_candidates(self, text):\n        \"\"\"\n        Uses NLTK to tokenize text, assign POS tags, and extract candidate phrases\n        based on sequences of adjectives (JJ) or nouns (NN).\n        \"\"\"\n        tokens = word_tokenize(text)\n        tagged = pos_tag(tokens)\n        candidates = []\n        candidate = []\n        for word, tag in tagged:\n            if tag.startswith(\"JJ\") or tag.startswith(\"NN\"):\n                candidate.append(word)\n            else:\n                if candidate:\n                    phrase = \" \".join(candidate)\n                    candidates.append(phrase)\n                    candidate = []\n        if candidate:\n            phrase = \" \".join(candidate)\n            candidates.append(phrase)\n        # Remove duplicates and keep phrases with at least one word\n        candidates = list(set([c for c in candidates if len(c.split()) >= 1]))\n        return candidates\n\n    def mask_text(self, text, candidate):\n        \"\"\"\n        Replaces occurrences of candidate in text with [MASK] tokens.\n        \"\"\"\n        candidate_tokens = candidate.split()\n        mask_token = \" \".join([\"[MASK]\"] * len(candidate_tokens))\n        # Use regex for case-insensitive replacement\n        pattern = re.compile(re.escape(candidate), re.IGNORECASE)\n        masked_text = pattern.sub(mask_token, text)\n        return masked_text\n\n    def cosine_similarity(self, vec1, vec2):\n        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n\n    def rank_keyphrases(self, text):\n        \"\"\"\n        Computes the embedding for the original text and, for each candidate keyphrase,\n        computes the embedding for the masked text. The candidate whose masking causes a\n        larger drop in cosine similarity (i.e. lower similarity) is considered more important.\n        \"\"\"\n        original_embedding = self.compute_embedding(text)\n        candidates = self.extract_candidates(text)\n        scores = {}\n        for candidate in candidates:\n            masked_text = self.mask_text(text, candidate)\n            masked_embedding = self.compute_embedding(masked_text)\n            sim = self.cosine_similarity(original_embedding, masked_embedding)\n            scores[candidate] = sim\n        # Sort candidates by increasing similarity\n        ranked = sorted(scores.items(), key=lambda x: x[1])\n        return ranked","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:55.287988Z","iopub.execute_input":"2025-03-24T04:39:55.288495Z","iopub.status.idle":"2025-03-24T04:39:55.575471Z","shell.execute_reply.started":"2025-03-24T04:39:55.288460Z","shell.execute_reply":"2025-03-24T04:39:55.574469Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def extract_keyphrases(text, top_k=10):\n    \"\"\"\n    Extract keyphrases from the input text. Returns the top_k keyphrases with the lowest similarity scores.\n    \"\"\"\n    mde = MDERank()\n    ranked = mde.rank_keyphrases(text.lower())\n    top_candidates = [phrase for phrase, score in ranked[:top_k]]\n    return top_candidates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:55.576671Z","iopub.execute_input":"2025-03-24T04:39:55.577043Z","iopub.status.idle":"2025-03-24T04:39:55.581273Z","shell.execute_reply.started":"2025-03-24T04:39:55.576986Z","shell.execute_reply":"2025-03-24T04:39:55.580508Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def clean_labels(labels):\n    clean_labels = {}\n    for id in labels:\n        label = labels[id]\n        clean_label = []\n        for kp in label:\n            if \";\" in kp:\n                left, right = kp.split(\";\")\n                clean_label.append(left)\n                clean_label.append(right)\n            else:\n                clean_label.append(kp)\n        clean_labels[id] = clean_label        \n    return clean_labels\n\ndef get_long_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/nus/nus_test.json\"):\n    \"\"\"Load file.jsonl for long documents.\"\"\"\n    data = {}\n    labels = {}\n    with codecs.open(file_path, 'r', 'utf-8') as f:\n        json_text = f.readlines()\n        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n            try:\n                jsonl = json.loads(line)\n                keywords = jsonl['keywords'].lower().split(\";\")\n                abstract = jsonl['abstract']\n                fulltxt = jsonl['fulltext']\n                doc = ' '.join([abstract, fulltxt])\n                doc = re.sub('\\. ', ' . ', doc)\n                doc = re.sub(', ', ' , ', doc)\n                doc = doc.replace('\\n', ' ')\n                data[jsonl['name']] = doc\n                labels[jsonl['name']] = keywords\n            except Exception as e:\n                raise ValueError(f\"Error processing line {i}: {e}\")\n    labels = clean_labels(labels)\n    return data, labels\n\ndef get_duc2001_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/DUC2001\"):\n    pattern = re.compile(r'<TEXT>(.*?)</TEXT>', re.S)\n    data = {}\n    labels = {}\n    for dirname, dirnames, filenames in os.walk(file_path):\n        for fname in filenames:\n            if fname == \"annotations.txt\":\n                infile = os.path.join(dirname, fname)\n                with open(infile, 'rb') as f:\n                    text = f.read().decode('utf8')\n                    lines = text.splitlines()\n                    for line in lines:\n                        left, right = line.split(\"@\")\n                        d = right.split(\";\")[:-1]\n                        labels[left] = d\n            else:\n                infile = os.path.join(dirname, fname)\n                with open(infile, 'rb') as f:\n                    text = f.read().decode('utf8')\n                    text = re.findall(pattern, text)[0]\n                    data[fname] = text\n    labels = clean_labels(labels)\n    return data, labels\n\ndef get_inspec_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/Inspec\"):\n    data = {}\n    labels = {}\n    for dirname, dirnames, filenames in os.walk(file_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            if right == \"abstr\":\n                infile = os.path.join(dirname, fname)\n                with open(infile, 'r', encoding='utf8') as f:\n                    text = f.read()\n                    text = text.replace(\"%\", '')\n                    data[left] = text\n            if right == \"uncontr\":\n                infile = os.path.join(dirname, fname)\n                with open(infile, 'r', encoding='utf8') as f:\n                    text = f.read()\n                    text = text.replace(\"\\n\\t\", ' ').replace(\"\\n\", ' ')\n                    label = text.split(\"; \")\n                    labels[left] = label\n    labels = clean_labels(labels)\n    return data, labels\n\ndef get_semeval2017_data(data_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2017/docsutf8\",\n                         labels_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2017/keys\"):\n    data = {}\n    labels = {}\n    for dirname, dirnames, filenames in os.walk(data_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            infile = os.path.join(dirname, fname)\n            with codecs.open(infile, \"r\", \"utf-8\") as fi:\n                text = fi.read()\n                text = text.replace(\"%\", '')\n            data[left] = text.lower()\n    for dirname, dirnames, filenames in os.walk(labels_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            infile = os.path.join(dirname, fname)\n            with open(infile, 'rb') as f:\n                text = f.read().decode('utf8').strip()\n                ls = text.splitlines()\n                labels[left] = ls\n    labels = clean_labels(labels)\n    return data, labels\n\ndef get_short_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/krapivin/kravipin_test.json\"):\n    \"\"\"Load file.jsonl for short documents.\"\"\"\n    data = {}\n    labels = {}\n    with codecs.open(file_path, 'r', 'utf-8') as f:\n        json_text = f.readlines()\n        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n            try:\n                jsonl = json.loads(line)\n                keywords = jsonl['keywords'].lower().split(\";\")\n                abstract = jsonl['abstract']\n                doc = abstract\n                doc = re.sub('\\. ', ' . ', doc)\n                doc = re.sub(', ', ' , ', doc)\n                doc = doc.replace('\\n', ' ').replace('\\t', ' ')\n                data[i] = doc\n                labels[i] = keywords\n            except Exception as e:\n                raise ValueError(f\"Error processing line {i}: {e}\")\n    labels = clean_labels(labels)\n    return data, labels\n\ndef get_krapivin_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/krapivin/krapivin_test.json\"):\n    return get_short_data(file_path)\n\ndef get_nus_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/nus/nus_test.json\"):\n    return get_long_data(file_path)\n\ndef get_semeval2010_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2010/semeval_test.json\"):\n    return get_short_data(file_path)\n\ndef get_dataset_data(dataset_name):\n    if dataset_name == \"duc2001\":\n        return get_duc2001_data()\n    elif dataset_name == \"inspec\":\n        return get_inspec_data()\n    elif dataset_name == \"krapivin\":\n        return get_krapivin_data()\n    elif dataset_name == \"nus\":\n        return get_nus_data()\n    elif dataset_name == \"semeval2010\":\n        return get_semeval2010_data()\n    elif dataset_name == \"sameval2017\":\n        return get_semeval2017_data()\n    else:\n        raise ValueError(\"Dataset name not recognized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:55.582794Z","iopub.execute_input":"2025-03-24T04:39:55.583063Z","iopub.status.idle":"2025-03-24T04:39:55.602410Z","shell.execute_reply.started":"2025-03-24T04:39:55.583037Z","shell.execute_reply":"2025-03-24T04:39:55.601668Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ndef calculate_f1(predicted, ground_truth, k) -> float:\n    \"\"\"\n    Calculate F1@K.\n    \n    Parameters:\n      predicted (list): List of predicted keyphrases.\n      ground_truth (list): List of ground truth keyphrases.\n      k (int): The cutoff for evaluation.\n    \n    Returns:\n      float: F1 score (scaled to percentage if desired).\n    \"\"\"\n    predicted_top_k = predicted[:k]\n    common = set(predicted_top_k) & set(ground_truth)\n    precision = len(common) * 1.0 / k if k > 0 else 0\n    recall = len(common) * 1.0 / len(ground_truth) if ground_truth else 0\n    f1 = 0\n    if precision + recall > 0:\n       f1 = 200.0 * precision * recall / (precision + recall)  # Multiply by 100*2 to get percentage\n    return f1\n\ndef print_to_json(data_name, k, score):\n    \"\"\"\n    Print the evaluation results to a JSON file.\n    \"\"\"\n    average_score = sum(score) / len(score) if score else 0\n    result = {\n        \"dataset\": data_name,\n        \"top_k\": k,\n        \"average_score\": average_score,\n    }\n    # Ensure the output directory exists; here, we create a folder \"working\" inside the current directory.\n    os.makedirs(\"/kaggle/working/mderank\", exist_ok=True)\n    print(\"---> f1@k_score: \", result)\n    with open(f\"/kaggle/working/mderank/{data_name}_{k}.json\", \"w\") as outfile:\n        json.dump(result, outfile)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:55.603337Z","iopub.execute_input":"2025-03-24T04:39:55.603609Z","iopub.status.idle":"2025-03-24T04:39:55.615849Z","shell.execute_reply.started":"2025-03-24T04:39:55.603587Z","shell.execute_reply":"2025-03-24T04:39:55.615096Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Main execution block\nif __name__ == \"__main__\":\n    # List the datasets you want to process. \n    dataset = [ 'semeval2010']\n    \n    for data_name in dataset:\n        print(data_name)\n        data, labels = get_dataset_data(data_name) \n        score5 = []\n        score10 = []\n        score15 = []\n        for id in data:\n            keyphrases = extract_keyphrases(data[id], top_k=15)\n            \n            f1_5 = calculate_f1(keyphrases, labels[id], 5)\n            f1_10 = calculate_f1(keyphrases, labels[id], 10)\n            f1_15 = calculate_f1(keyphrases, labels[id], 15)\n            score5.append(f1_5)\n            score10.append(f1_10)\n            score15.append(f1_15)\n            \n            print(\"**** \", id, \" ---> \", f1_5, ' ', f1_10, ' ', f1_15)\n            if (f1_5 == 0): \n                print(data[id])\n                print(\"++++++ label: \", labels[id])\n                print(\"++++++ model:\", keyphrases)\n        print_to_json(data_name, 5, score5)\n        print_to_json(data_name, 10, score10)\n        print_to_json(data_name, 15, score15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:39:55.616617Z","iopub.execute_input":"2025-03-24T04:39:55.616878Z","iopub.status.idle":"2025-03-24T04:58:17.224223Z","shell.execute_reply.started":"2025-03-24T04:39:55.616848Z","shell.execute_reply":"2025-03-24T04:58:17.223271Z"}},"outputs":[{"name":"stdout","text":"semeval2010\n","output_type":"stream"},{"name":"stderr","text":"Loading Doc ...: 100it [00:00, 8102.59it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e9a7bf8b88344cab6a2efb371fd1497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b05a724df5412288c9ad4291d0345c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"412711c062cf40fd86d6cb40b1fb6e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8e4cf869234036b03eb914b57a2888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a139ac79ed40fdb7d99febc6b66f01"}},"metadata":{}},{"name":"stdout","text":"****  0  --->  8.333333333333332   20.689655172413794   17.647058823529413\n****  1  --->  21.052631578947363   25.0   34.48275862068965\n****  2  --->  28.571428571428573   23.076923076923077   19.35483870967742\n****  3  --->  10.526315789473681   16.666666666666664   13.793103448275861\n****  4  --->  0   0   0\nIn this paper we compare two approaches to the design of protocol frameworks - tools for implementing modular network protocols . The most common approach uses events as the main abstraction for a local interaction between protocol modules . We argue that an alternative approach , that is based on service abstraction , is more suitable for expressing modular protocols . It also facilitates advanced features in the design of protocols , such as dynamic update of distributed protocols . We then describe an experimental implementation of a service-based protocol framework in Java.\n++++++ label:  ['communication', 'request', 'modularity', 'dynamic protocol replacement', 'service interface', 'event-based framework', 'stack', 'distributed system', 'protocol framework', 'reply', 'module', 'network', 'distributed algorithm']\n++++++ model: ['service-based protocol framework', 'protocols', 'modular network protocols', 'protocol frameworks', 'alternative approach', 'service abstraction', 'design', 'experimental implementation', 'main abstraction', 'modular protocols', 'distributed protocols', 'advanced features', 'dynamic update', 'protocol modules', 'local interaction']\n****  5  --->  0   0   7.142857142857144\nA significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages , whether planned or unplanned . In this paper we advocate a cooperative , context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner . We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities . We make use of server virtualization technologies to enable the replication and migration of server functions . We propose new network functions to enable server migration and replication across wide area networks (e.g. , the Internet) , and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.\n++++++ label:  ['internet-based service', 'data center migration', 'network support', 'voip', 'storage replication', 'synchronous replication', 'asynchronous replication', 'lan', 'wan', 'database', 'storage', 'virtual server', 'voice-over-ip']\n++++++ model: ['outages', 'dynamic storage replication technology', 'data', 'internet-based service providers', 'server virtualization technologies', 'wide area networks', 'tight recovery point objectives', 'new network functions', 'face', 'replication', 'data center facilities', 'data center services', 'data center migration', 'unanticipated outages', 'non-disruptive manner']\n****  6  --->  10.526315789473681   16.666666666666664   13.793103448275861\n****  7  --->  9.523809523809524   7.692307692307692   6.451612903225808\n****  8  --->  21.052631578947363   16.666666666666664   20.689655172413794\n****  9  --->  0   7.999999999999999   13.333333333333334\nIn this paper , we propose an adaptive task allocation framework to perform BLAST searches in a grid environment against sequence database segments . The framework , called PackageBLAST , provides an infrastructure to choose or incorporate task allocation strategies . Furthermore , we propose a mechanism to compute grid nodes execution weight , adapting the chosen allocation policy to the current computational power of the nodes . Our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios.\n++++++ label:  ['blast search', 'bioinformatic', 'heterogeneous non-dedicated platform', 'segmented genetic database', 'grid environment', 'packageblast', 'grid computing', 'task allocation', 'pss', 'package weighted adaptive self-scheduling', 'adaptive multi-policy grid service', 'genome project', 'biological sequence comparison', 'bioinformatics', 'computational biology']\n++++++ model: ['grid nodes execution weight', 'adaptive task allocation framework', 'lowest execution times', 'current computational power', 'sequence database segments', 'task allocation strategies', 'chosen allocation policy', 'single allocation strategy', 'nodes', 'grid environment', 'framework', 'results present', 'blast searches', 'good speedups', 'packageblast']\n****  10  --->  11.11111111111111   17.391304347826086   14.285714285714288\n****  11  --->  0   0   6.0606060606060606\nGrids are inherently heterogeneous and dynamic . One important problem in grid computing is resource selection , that is , finding an appropriate resource set for the application . Another problem is adaptation to the changing characteristics of the grid environment . Existing solutions to these two problems require that a performance model for an application is known . However , constructing such models is a complex task . In this paper , we investigate an approach that does not require performance models . We start an application on any set of resources . During the application run , we periodically collect the statistics about the application run and deduce application requirements from these statistics . Then , we adjust the resource set to better fit the application needs . This approach allows us to avoid performance bottlenecks , such as overloaded WAN links or very slow processors , and therefore can yield significant performance improvements . We evaluate our approach in a number of scenarios typical for the Grid.\n++++++ label:  ['resource selection', 'divide-and-conquer', 'self-adaptivity', 'network link', 'lower-bandwidth wide-area network', 'grid environment', 'degree of parallelism', 'grid computing', 'communication time', 'heterogeneity of resource', 'parallelism degree', 'idle time of the processor', 'overloaded resource', 'homogeneous parallel environment', 'the processor idle time', 'high-bandwidth local-area network', 'parallel computing', 'resource heterogeneity']\n++++++ model: ['grid', 'application', 'performance model', 'resource set', 'application run', 'set', 'overloaded wan links', 'problem', 'approach', 'significant performance improvements', 'scenarios typical', 'deduce application requirements', 'slow processors', 'resource selection', 'performance bottlenecks']\n****  12  --->  9.523809523809524   15.384615384615383   25.80645161290323\n****  13  --->  10.0   7.999999999999999   6.666666666666667\n****  14  --->  10.0   7.999999999999999   6.666666666666667\n****  15  --->  12.500000000000002   19.047619047619047   23.076923076923077\n****  16  --->  0   18.18181818181818   14.814814814814817\nSecurity schemes of pairwise key establishment , which enable sensors to communicate with each other securely , play a fundamental role in research on security issue in wireless sensor networks . A new kind of cluster deployed sensor networks distribution model is presented , and based on which , an innovative Hierarchical Hypercube model - H(k,u,m,v,n) and the mapping relationship between cluster deployed sensor networks and the H(k,u,m,v,n) are proposed . By utilizing nice properties of H(k,u,m,v,n) model , a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed , which combines the idea of KDC(Key Distribution Center) and polynomial pool schemes . Furthermore , the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected . Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key , compared with previous related works.\n++++++ label:  ['key pool', 'node code', 'high fault-tolerance', 'hierarchical hypercube model', 'pairwise key establishment algorithm', 'cluster-based distribution model', 'pairwise key', 'encryption', 'sensor network', 'polynomial key', 'security', 'key predistribution']\n++++++ model: ['n', 'm', 'h', 'u', 'k', 'pairwise key establishment algorithm', 'pairwise key', 'pairwise key establishment', 'v', 'sensor networks', 'sensor', 'new pairwise key establishment algorithm', 'works', 'sensor networks distribution model', 'key distribution center']\n****  17  --->  0   0   0\nPublish/subscribe systems provide an efficient , event-based , wide-area distributed communications infrastructure . Large scale publish/subscribe systems are likely to employ components of the event transport network owned by cooperating , but independent organisations . As the number of participants in the network increases , security becomes an increasing concern . This paper extends previous work to present and evaluate a secure multi-domain publish/subscribe infrastructure that supports and enforces fine-grained access control over the individual attributes of event types . Key refresh allows us to ensure forward and backward security when event brokers join and leave the network . We demonstrate that the time and space overheads can be minimised by careful consideration of encryption techniques , and by the use of caching to decrease unnecessary decryptions . We show that our approach has a smaller overall communication overhead than existing approaches for achieving the same degree of control over security in publish/subscribe networks.\n++++++ label:  ['secure publish/subscribe system', 'distribute access control', 'multi-domain', 'performance', 'distributed access control', 'distributed systems-distributed application', 'overall communication overhead', 'encryption', 'multiple administrative domain', 'administrative domain', 'congestion charge service', 'attribute encryption']\n++++++ model: ['publish/subscribe systems', 'wide-area distributed communications infrastructure', 'secure multi-domain publish/subscribe infrastructure', 'network', 'large scale publish/subscribe systems', 'smaller overall communication overhead', 'fine-grained access control', 'publish/subscribe networks', 'event brokers join', 'security', 'event transport network', 'approach', 'same degree', 'control', 'unnecessary decryptions']\n****  18  --->  11.11111111111111   17.391304347826086   14.285714285714288\n****  19  --->  7.407407407407407   6.250000000000001   5.405405405405405\n****  20  --->  0   8.333333333333332   6.896551724137931\nNewly emerging game-based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real-time . They are filled with autonomous , mutable virtual content which is continuously augmented by the users . To make the systems highly scalable and dynamically extensible , they are usually built on a client-server based grid subspace division where the virtual worlds are partitioned into manageable sub-worlds . In each sub-world , the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint , rather than retrieving them from a local storage medium . In such systems , the determination of the set of objects that are visible from a user\"s viewpoint is one of the primary factors that affect server throughput and scalability . Specifically , performing real-time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting . We recognize that the described challenges are closely related to a spatial database problem , and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query . Unfortunately , existing spatial indexing methods are unsuitable for this kind of new environments . The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real-time visibility determination . We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality , system throughput , and resource utilization . We expect our approach to lay the groundwork for next-generation virtual frameworks that may merge into existing web-based services in the near future.\n++++++ label:  ['dynamic virtual environment', 'mutable virtual content', 'real-time visibility test', '3d object stream', 'object pop problem', 'object popping', 'visibility model', 'game-based application', 'object-initiated view model', 'spatial database', 'edge indexing', 'spatial index', '3d spatial extension', 'spatial indexing method']\n++++++ model: ['user', 'spatial indexing methods', 'game-based application systems such', 'objects', 'next-generation virtual frameworks', 'index structure', 'virtual environments', 'efficient spatial index structure', 'spatial database', 'scalable real-time visibility determination', 'real-time visibility tests', 'dynamic virtual environments', 'real-time', 'grid subspace division', 'spatial database problem']\n****  21  --->  0   9.523809523809524   7.692307692307692\nThe convergence of advances in storage , encoding , and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices . Keeping track of (or managing) such content remains challenging due to the sheer volume of data . Storing live continuous media (such as TV or radio content) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with . Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network , making the task of content management all but impossible to deal with without a content management system . In this paper we present the design and implementation of the Spectrum content management system , which deals with rich media content effectively in this environment . Spectrum has a modular architecture that allows its application to both stand-alone and various networked scenarios . A unique aspect of Spectrum is that it requires one (or more) retention policies to apply to every piece of content that is stored in the system . This means that there are no eviction policies . Content that no longer has a retention policy applied to it is simply removed from the system . Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication . This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media , to content . We not only describe the details of the Spectrum architecture but also give typical use cases.\n++++++ label:  ['network enabled dvr', 'policy manager', 'uniform resource locator', 'spectrum content management system', 'distribute content management', 'carrier-grade spectrum manager', 'high-performance database system', 'continuous media storage', 'application program interface', 'home-networking scenario', 'content distribution network']\n++++++ model: ['content', 'live continuous media', 'spectrum', 'content management system', 'network', 'spectrum content management system', 'content management', 'retention policies', 'various networked scenarios', 'system', 'policies', 'storage', 'typical use cases', 'rich media content', 'different retention policies']\n****  22  --->  14.814814814814815   12.500000000000002   16.216216216216214\n****  23  --->  0   0   0\nWhile market-based systems have long been proposed as solutions for distributed resource allocation , few have been deployed for production use in real computer systems . Towards this end , we present our initial experience using Mirage , a microeconomic resource allocation system based on a repeated combinatorial auction . Mirage allocates time on a heavily-used 148-node wireless sensor network testbed . In particular , we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects . Based on these results , we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions . Finally , we propose refinements to the system\"s current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction.\n++++++ label:  ['']\n++++++ model: ['combinatorial auction', 'heavily-used 148-node wireless sensor network', 'resource allocation', 'microeconomic resource allocation system', 's current auction scheme', 'system', 'market-based resource allocation systems', 'observed strategic user behavior', 'real computer systems', 'market-based systems', 'combinatorial auctions', 'initial experience', 'research projects', 'initial steps', 'four-month period']\n****  24  --->  0   0   0\nAs the idea of virtualisation of compute power , storage and bandwidth becomes more and more important , grid computing evolves and is applied to a rising number of applications . The environment for decentralized adaptive services (EDAS) provides a grid-like infrastructure for user-accessed , longterm services (e.g . webserver , source-code repository etc.) . It aims at supporting the autonomous execution and evolution of services in terms of scalability and resource-aware distribution . EDAS offers flexible service models based on distributed mobile objects ranging from a traditional clientserver scenario to a fully peer-to-peer based approach . Automatic , dynamic resource management allows optimized use of available resources while minimizing the administrative complexity.\n++++++ label:  ['local limit', 'client', 'node', 'decentralized adaptive service', 'home environment', 'infrastructure', 'grid computing', 'global limit', 'adaptability', 'long-term service', 'resource', 'fragment object', 'resource management', 'eda']\n++++++ model: ['services', 'edas', 'source-code repository etc', 'dynamic resource management', 'traditional clientserver scenario', 'flexible service models', 'grid', 'longterm services', 'administrative complexity', 'grid-like infrastructure', 'compute power', 'use', 'resource-aware distribution', 'bandwidth becomes', 'adaptive services']\n****  25  --->  11.76470588235294   9.09090909090909   7.407407407407408\n****  26  --->  11.11111111111111   17.391304347826086   21.42857142857143\n****  27  --->  10.526315789473681   8.333333333333332   6.896551724137931\n****  28  --->  11.11111111111111   17.391304347826086   14.285714285714288\n****  29  --->  0   8.695652173913043   7.142857142857144\nWe present a novel Web search interaction feature which , for a given query , provides links to websites frequently visited by other users with similar information needs . These popular destinations complement traditional search results , allowing direct navigation to authoritative resources for the query topic . Destinations are identified using the history of search and browsing behavior of many users over an extended time period , whose collective behavior provides a basis for computing source authority . We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries , as well as with traditional , unaided Web search . Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks , with best performance obtained from mining past user behavior at query-level granularity.\n++++++ label:  ['web search interaction', 'search destination', 'popular destination', 'information-seeking experience', 'retrieval performance', 'improving query', 'user study', 'log-based evaluation', 'enhance web search', 'session trail', 'related query', 'lookup-based approach', 'query trail']\n++++++ model: ['novel web search interaction feature', 'search', 'destinations', 'similar information needs', 'extended time period', 'unaided web search', 'traditional search results', 'behavior', 'query', 'user study', 'query-level granularity', 'related queries', 'suggestion', 'collective behavior', 'user behavior']\n****  30  --->  16.666666666666664   20.689655172413794   17.647058823529413\n****  31  --->  9.523809523809524   15.384615384615383   19.35483870967742\n****  32  --->  8.0   26.666666666666664   28.57142857142857\n****  33  --->  0   0   6.896551724137931\nWe consider the problem of analyzing word trajectories in both time and frequency domains , with the specific goal of identifying important and less-reported , periodic and aperiodic words . A set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner . The document frequency of each word across time is treated like a time series , where each element is the document frequency - inverse document frequency (DFIDF) score at one time point . In this paper , we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported , periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities , and subsequently detected each feature\"s burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events . All of the above methods can be applied to time series data in general . We extensively evaluated our methods on the 1-year Reuters News Corpus [3] and showed that they were able to uncover meaningful aperiodic and periodic events.\n++++++ label:  ['feature categorization', 'gaussian', 'dft', 'aperiodic event', 'text stream', 'topic tracking', 'word signal', 'topic detection', 'time series', 'news stream', 'periodic event', 'word trajectory', 'event detection', 'spectral analysis']\n++++++ model: ['s', 'periodic', 'unsupervised greedy event detection algorithm', 'aperiodic', '1-year reuters news corpus', 'time', 'event', 'document frequency', 'truncated gaussian approach', 'word', 'less-reported', 'time series', 'time series data', 'periodic events', 'gaussian mixture densities']\n****  34  --->  6.451612903225806   16.666666666666668   19.512195121951223\n****  35  --->  7.692307692307692   6.451612903225806   11.11111111111111\n****  36  --->  0   0   0\nWe propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy , while dealing in realtime with the query volume of a commercial web search engine . We use a blind feedback technique: given a query , we determine its topic by classifying the web search results retrieved by the query . Motivated by the needs of search advertising , we primarily focus on rare queries , which are the hardest from the point of view of machine learning , yet in aggregation account for a considerable fraction of search engine traffic . Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported . We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.\n++++++ label:  ['blind relevance feedback', 'conditional probability', 'relevance feedback', 'affinity score', 'web search', 'voting scheme', 'search advertising', 'crawling', 'topical taxonomy', 'information retrieval', 'machine learning', 'search engine', 'adaptation', 'query classification']\n++++++ model: ['rare queries', 'query', 'better user experience', 'practical robust query classification system', 'commercial web search engine', 'blind feedback technique', 'web search results', 'search engine traffic', 'online ads', 'higher classification accuracy', 'methodology', 'queries', 'better matching', 'reasonable accuracy', 'aggregation account']\n****  37  --->  0   0   6.896551724137931\nOne way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing , and use this knowledge to benefit everyone . In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search . The results show that there are marked differences in the queries , result clicks , post-query browsing , and search success of users we classify as advanced (based on their use of query operators) , relative to those classified as non-advanced . Our findings have implications for how advanced users should be supported during their searches , and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.\n++++++ label:  ['relevance', 'relevance feedback', 'navigation behavior', 'expert search', 'search behavior', 'advance search feature', 'searching strategy', 'advanced syntax', 'query', 'query syntax', 'search engine', 'search success', 'tolerable latency', 'relevant information']\n++++++ model: ['use', 'advanced', 'users', 'commercial web search engines', 'advanced search engine users', 'greater search expertise', 'user groups search', 'post-query browsing', 'experience levels', 'query operators', 'search success', 'interaction logs', 'marked differences', 'searches', 'better']\n****  38  --->  23.52941176470588   27.27272727272727   22.22222222222222\n****  39  --->  20.0   15.999999999999998   20.0\n****  40  --->  11.11111111111111   8.695652173913043   14.285714285714288\n****  41  --->  0   0   0\nUser query is an element that specifies an information need , but it is not the only one . Studies in literature have found many contextual factors that strongly influence the interpretation of a query . Recent studies have tried to consider the user\"s interests by creating a user profile . However , a single profile for a user may not be sufficient for a variety of queries of the user . In this study , we propose to use query-specific contexts instead of user-centric ones , including context around query and context within query . The former specifies the environment of a query such as the domain of interest , while the latter refers to context words within the query , which is particularly useful for the selection of relevant term relations . In this paper , both types of context are integrated in an IR model based on language modeling . Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.\n++++++ label:  ['knowledge ambiguity problem', 'term relation', 'interest domain', 'domain of interest', 'query context', 'information need', 'radical solution', 'domain knowledge', 'utilization of general knowledge', 'general knowledge utilization', 'context-independent', 'user-centric one', 'context information', 'domain model', 'context factor', 'google personalized search', 'user profile', 'language model', 'problem of knowledge ambiguity', 'search context', 'word sense disambiguation', 'query-specific context']\n++++++ model: ['context', 'query', 'user', 'context factors', 'many contextual factors', 'several trec collections', 'relevant term relations', 'retrieval effectiveness', 'significant improvements', 'interest', 'latter refers', 'language modeling', 'only one', 'ir model', 's interests']\n****  42  --->  9.09090909090909   14.814814814814817   18.750000000000004\n****  43  --->  9.09090909090909   7.407407407407408   12.5\n****  44  --->  8.333333333333332   6.896551724137931   11.76470588235294\n****  45  --->  23.52941176470588   18.18181818181818   14.814814814814817\n****  46  --->  9.09090909090909   7.407407407407408   12.5\n****  47  --->  0   7.999999999999999   6.666666666666667\nA content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user\"s interest . A system serving millions of users can learn a better user profile for a new user , or a user with little feedback , by borrowing information from other users through the use of a Bayesian hierarchical model . Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive . The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications . This paper proposes a new fast learning technique to learn a large number of individual user profiles . The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.\n++++++ label:  ['classification', 'information filter', 'collaborative filtering', 'ir', 'learning technique', 'recommender system', 'bayesian hierarchical model', 'recommendation system', 'rating', 'content-based', 'em algorithm', 'personalization', 'linear regression', 'modeling', 'parameter']\n++++++ model: ['use', 'user', 'new fast learning technique', 'individual user', 'users', 'bayesian hierarchical model', 'em algorithm converges', 'better user profile', 'actual user data', 'individual user profiles', 'joint data likelihood', 's interest', 'data', 'millions', 'algorithm']\n****  48  --->  0   9.09090909090909   7.407407407407408\nLow-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments . While these judgments are very useful for a one-time evaluation , it is not clear that they can be trusted when re-used to evaluate new systems . In this work , we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments . We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort . Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems , we can reliably evaluate a larger set of ten systems . Even the smallest sets of judgments can be useful for evaluation of new systems.\n++++++ label:  ['evaluation', 'test collection', 'relevance judgement', 'variance', 'distribution of relevance', 'rtc', 'reusability', 'expectation', 'information retrieval', 'relevance distribution', 'mtc', 'lowerest-confidence comparison']\n++++++ model: ['judgments', 'relevance judgments', 'new systems', 'systems', 'set', 'additional assessor effort', 'new retrieval tasks', 'evaluation', 'method', 'low-cost methods', 'smallest sets', 'larger set', 'useful', 'topic', 'ten systems']\n****  49  --->  0   0   0\nEffective organization of search results is critical for improving the utility of any search engine . Clustering search results is an effective way to organize search results , which allows a user to navigate into relevant documents quickly . However , two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user\"s perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster . In this paper , we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users . We evaluate our proposed method on a commercial search engine log data . Compared with the traditional methods of clustering search results , our method can give better result organization and more meaningful labels.\n++++++ label:  ['interest aspect', 'ranking function', 'history collection', 'clickthrough', 'centroid prototype', 'monothetic clustering algorithm', 'reciprocal rank', 'centroid-based method', 'search result organization', 'mean average precision', 'clustering view', 'past query', 'cosine similarity', 'similarity threshold parameter', 'log-based method', 'meaningful cluster label', 'pseudo-document', 'retrieval model', 'ambiguity', 'search engine log', 'star clustering algorithm', 'search result snippet', 'pairwise similarity graph', 'suffix tree clustering algorithm']\n++++++ model: ['search results', 'commercial search engine log data', 'user', 'web search logs', 'interesting aspects', 'search engine', 'cluster labels', 'better result organization', 'meaningful cluster labels', 'method', 'past query words', 's perspective', 'meaningful labels', 'deficiencies', 'topic']\n****  50  --->  22.22222222222222   17.391304347826086   14.285714285714288\n****  51  --->  13.333333333333332   10.0   8.0\n****  52  --->  8.333333333333332   6.896551724137931   11.76470588235294\n****  53  --->  12.500000000000002   9.523809523809524   7.692307692307692\n****  54  --->  0   0   0\nThe dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches . In these approaches , agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions . However , such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents . In this paper , an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions . Specifically , agents maintain estimates on the downstream agents\" abilities to provide relevant documents for incoming queries . These estimates are updated gradually by learning from the feedback information returned from previous search sessions . Based on this information , the agents derive corresponding routing policies . Thereafter , these agents route the queries based on the learned policies and update the estimates based on the new routing policies . Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.\n++++++ label:  ['peer-to-peer information retrieval', 'distributed search algorithm', 'reinforcement learning', 'distribute search control', 'routing decision', 'routing policy', 'learning algorithm', 'utility', 'peer-to-peer information retrieval system', 'query', 'multi-agent learn', 'network']\n++++++ model: ['ir', 'agents', 'search sessions', 'queries', 'information', 'dynamic run-time characteristics', 'p2p ir systems', 'distributed ir studies', 'policies', 'p2p', 'new routing policies', 'neighboring agents', 'approach', 'past search sessions', 'previous search sessions']\n****  55  --->  0   0   0\nFinding the right agents in a large and dynamic network to provide the needed resources in a timely fashion , is a long standing problem . This paper presents a method for information searching and sharing that combines routing indices with tokenbased methods . The proposed method enables agents to search effectively by acquiring their neighbors\" interests , advertising their information provision abilities and maintaining indices for routing queries , in an integrated way . Specifically , the paper demonstrates through performance experiments how static and dynamic networks of agents can be tuned\" to answer queries effectively as they gather evidence for the interests and information provision abilities of others , without altering the topology or imposing an overlay structure to the network of acquaintances.\n++++++ label:  ['decentralized partially-observable markov decision process', 'artificial social system', 'gradient search scheme', 'performance', 'scalability', 'robustness', 'cooperative agent', 'myopic algorithm', 'knn approach', 'peer to peer search network', 'dependability', 'social network', 'decentralized control', 'dynamic and large scale network', 'information searching and sharing', 'peer-to-peer system']\n++++++ model: ['information provision abilities', 'dynamic network', 'queries', 'network', 'paper', 'agents', 'long standing problem', 'interests', 'method', 'dynamic networks', 'right agents', 'performance experiments', 'tokenbased methods', 'indices', 'information searching']\n****  56  --->  10.526315789473681   25.0   20.689655172413794\n****  57  --->  21.052631578947363   16.666666666666664   13.793103448275861\n****  58  --->  0   0   7.407407407407408\nWe derive optimal bidding strategies for a global bidding agent that participates in multiple , simultaneous second-price auctions with perfect substitutes . We first consider a model where all other bidders are local and participate in a single auction . For this case , we prove that , assuming free disposal , the global bidder should always place non-zero bids in all available auctions , irrespective of the local bidders\" valuation distribution . Furthermore , for non-decreasing valuation distributions , we prove that the problem of finding the optimal bids reduces to two dimensions . These results hold both in the case where the number of local bidders is known and when this number is determined by a Poisson distribution . This analysis extends to online markets where , typically , auctions occur both concurrently and sequentially . In addition , by combining analytical and simulation results , we demonstrate that similar results hold in the case of several global bidders , provided that the market consists of both global and local bidders . Finally , we address the efficiency of the overall market , and show that information about the number of local bidders is an important determinant for the way in which a global bidder affects efficiency.\n++++++ label:  ['global bidding agent', 'online market', 'social and behavioral science', 'perfect substitute', 'simultaneous auction', 'simultaneous second-price auction', 'utilitymaximising strategy', 'optimal bidding strategy', 'vickrey auction', 'non-decreasing valuation distribution', 'multiagent system', 'market efficiency']\n++++++ model: ['global bidder', 'local bidders', 'global', 'valuation distribution', 'local', 'several global bidders', 'number', 'simultaneous second-price auctions', 'market', 'auctions', 'non-decreasing valuation distributions', 'case', 'global bidding agent', 'results', 'optimal bidding strategies']\n****  59  --->  10.526315789473681   8.333333333333332   13.793103448275861\n****  60  --->  0   0   0\nWe investigate a framework where agents search for satisfying products by using referrals from other agents . Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents . Local behaviour incorporates a satisficing model of choice , a set of rules governing the interactions between agents , including learning about the trustworthiness of other agents over time , and external constraints on behaviour that may be imposed by market barriers or switching costs . Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products . We use various degree distributions dictating the extent of connectivity , and incorporate both small-world effects and the notion of preferential attachment in our network models . We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks , and evaluate how this effectiveness changes with the imposition of market barriers.\n++++++ label:  ['psychological affinity', 'switching cost', 'word-of-mouth communication', 'cognitive model', 'artificial social system', 'market barrier', 'referral system', 'switching behaviour', 'social psychology', 'social network', 'purchasing behaviour', 'agent-based model', 'consumer choice', 'marketing system', 'defection behaviour']\n++++++ model: ['agents', 'behaviour', 'local behaviour', 'hard choice tasks', 'various network structures', 'other agents', 'resulting behavioural effects', 'various degree distributions', 'market barriers', 'model', 'referral systems', 'structure', 'small-world effects', 'effectiveness', 'products']\n****  61  --->  0   0   0\nHuman team members often develop shared expectations to predict each other\"s needs and coordinate their behaviors . In this paper the concept Shared Belief Map is proposed as a basis for developing realistic shared expectations among a team of Human-Agent-Pairs (HAPs) . The establishment of shared belief maps relies on inter-agent information sharing , the effectiveness of which highly depends on agents\" processing loads and the instantaneous cognitive loads of their human partners . We investigate HMM-based cognitive load models to facilitate team members to share the right information with the right party at the right time . The shared belief map concept and the cognitive/processing load models have been implemented in a cognitive agent architectureSMMall . A series of experiments were conducted to evaluate the concept , the models , and their impacts on the evolving of shared mental models of HAP teams.\n++++++ label:  ['teamwork', 'cognitive model', 'collaboration', 'problem-solving', 'task performance', 'multi-party communication', 'human-agent team performance', 'reasoning', 'expectation', 'human performance', 'human-center teamwork', 'info-sharing', 'resource allocation', 'teamwork schema', 'heuristic', 'shared belief map', 'multiagent teamwork', 'share belief map', 'cognitive load theory']\n++++++ model: ['share', 'team', 'belief map', 'shared belief map concept', 'hmm-based cognitive load models', 'models', 'concept', 'load models', 'team members', 'inter-agent information sharing', 'belief maps relies', 'cognitive agent architecturesmmall', 'human team members', 'instantaneous cognitive loads', 'expectations']\n****  62  --->  0   0   5.714285714285714\nThis paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships . In each search stage , each of the agents is randomly matched with several other agents in parallel , and makes a decision whether to accept a potential partnership with one of them . The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized (instantaneous) decision protocol and can sequentially accept and reject partnerships within the same search stage . We analyze the dynamics which drive the agents\" strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model . By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments.\n++++++ label:  ['equilibrium strategy', 'sequential decision making', 'two-side search', 'bounding methodology', 'parallel interaction', 'multi-equilibrium scenario', 'coalition formation', 'partnership', 'information processing', 'costly environment', 'pairwise partnership', 'two-sided search', 'utility', 'search cost', 'decision', 'search performance', 'peer-to-peer application', 'partnership formation', 'match', 'instantaneous decision making']\n++++++ model: ['two-sided parallel economic search model', 'agents', 'two-sided economic search model', 'equilibrium', 'distributed equilibrium strategies', 'model', 'beneficial pairwise partnerships', 'search stage', 'several unique characteristics', 'several other agents', 'same search stage', 'decision', 'parallel', 'partnerships', 'decision protocol']\n****  63  --->  0   0   0\nWe consider the problem of managing schedules in an uncertain , distributed environment . We assume a team of collaborative agents , each responsible for executing a portion of a globally pre-established schedule , but none possessing a global view of either the problem or solution . The goal is to maximize the joint quality obtained from the activities executed by all agents , given that , during execution , unexpected events will force changes to some prescribed activities and reduce the utility of executing others . We describe an agent architecture for solving this problem that couples two basic mechanisms: (1) a flexible times representation of the agent\"s schedule (using a Simple Temporal Network) and (2) an incremental rescheduling procedure . The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions , and the latter acts to revise the agent\"s schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set . Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities . Then , as time permits , the core local problem solving infra-structure is used to drive an inter-agent option generation and query process , aimed at identifying opportunities for solution improvement through joint change . Using a simulator to model the environment , we compare the performance of our multi-agent system with that of an expected optimal (but non-scalable) centralized MDP solver.\n++++++ label:  ['shortest path algorithm', 'agent architecture', 'scheduler-execution', 'performance', 'slack', 'flexible time', 'management', 'conflict-driven approach', 'multi-agent schedule', 'inter-dependent activity', 'managing schedule', 'optimistic synchronization', 'inter-agent coordination', 'activity allocator', 'centralized planning', 'geographical separation', 'distributed environment', 'schedule']\n++++++ model: ['agent', 'solution', 's schedule', 'execution', 'problem', 'feasible solution', 'inter-agent option generation', 'agents', 'set', 'core local problem', 'activities', 'simple temporal network', 'flexible times representation', 'incremental rescheduling procedure', 'inter-dependent activities']\n****  64  --->  10.526315789473681   8.333333333333332   13.793103448275861\n****  65  --->  19.047619047619047   15.384615384615383   12.903225806451616\n****  66  --->  9.09090909090909   14.814814814814817   12.5\n****  67  --->  10.0   7.999999999999999   6.666666666666667\n****  68  --->  0   0   0\nNorm-governed virtual organizations define , govern and facilitate coordinated resource sharing and problem solving in societies of agents . With an explicit account of norms , openness in virtual organizations can be achieved: new components , designed by various parties , can be seamlessly accommodated . We focus on virtual organizations realised as multi-agent systems , in which human and software agents interact to achieve individual and global goals . However , any realistic account of norms should address their dynamic nature: norms will change as agents interact with each other and their environment . Due to the changing nature of norms or due to norms stemming from different virtual organizations , there will be situations when an action is simultaneously permitted and prohibited , that is , a conflict arises . Likewise , there will be situations when an action is both obliged and prohibited , that is , an inconsistency arises . We introduce an approach , based on first-order unification , to detect and resolve such conflicts and inconsistencies . In our proposed solution , we annotate a norm with the set of values their variables should not have in order to avoid a conflict or an inconsistency with another norm . Our approach neatly accommodates the domain-dependent interrelations among actions and the indirect conflicts/inconsistencies these may cause . More generally , we can capture a useful notion of inter-agent (and inter-role) delegation of actions and norms associated to them , and use it to address conflicts/inconsistencies caused by action delegation . We illustrate our approach with an e-Science example in which agents support Grid services.\n++++++ label:  ['conflicting prohibition', 'external agent', 'governor agent', 'norm conflict', 'agent', 'norm-regulated vo', 'norm inconsistency', 'virtual organization', 'artificial social systems', 'multi-agent system']\n++++++ model: ['norm', 'virtual organizations', 'conflict', 'norms', 'inconsistencies', 'agents support grid services', 'action', 'agents interact', 'norm-governed virtual organizations', 'agents', 'approach', 'inconsistency', 'inconsistency arises', 'first-order unification', 'conflicts/inconsistencies']\n****  69  --->  8.695652173913045   7.142857142857143   18.18181818181818\n****  70  --->  20.0   15.999999999999998   13.333333333333334\n****  71  --->  11.11111111111111   8.695652173913043   14.285714285714288\n****  72  --->  0   9.523809523809524   7.692307692307692\nThis paper proposes dynamic semantics for agent communication languages (ACLs) as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems . Based on the idea of providing alternative semantic variants for speech acts and transition rules between them that are contingent on previous agent behaviour , our framework provides an improved notion of grounding semantics in ongoing interaction , a simple mechanism for distinguishing between compliant and expected behaviour , and a way to specify sanction and reward mechanisms as part of the ACL itself . We extend a common framework for commitment-based ACL semantics to obtain these properties , discuss desiderata for the design of concrete dynamic semantics together with examples , and analyse their properties.\n++++++ label:  ['state transition system', 'commitment-based semantics', 'agent communication language', 'recovery mechanism', 'mutuality of expectation', 'expectation mutuality', 'non-redundancy', 'social reason', 'dynamic semantics', 'social reasoning', 'reputation-based adaptation']\n++++++ model: ['acl', 'agent communication', 'semantics', 'agent communication languages', 'commitment-based acl semantics', 'alternative semantic variants', 'dynamic semantics', 'open multiagent systems', 'previous agent behaviour', 'concrete dynamic semantics', 'fundamental problems', 'behaviour', 'framework', 'transition rules', 'common framework']\n****  73  --->  9.09090909090909   22.222222222222225   18.750000000000004\n****  74  --->  0   0   0\nInteractions between agents in an open system such as the Internet require a significant degree of flexibility . A crucial aspect of the development of such methods is the notion of commitments , which provides a mechanism for coordinating interactive behaviors among agents . In this paper , we investigate an approach to model commitments with tight integration with protocol actions . This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments . We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic , which incorporates both temporal and resource-sensitive reasoning . We also discuss the application of this framework to scenarios such as online commerce.\n++++++ label:  ['request message', 'classical conjunction', 'multiplicative conjunction', 'linear logic', 'multi-agent environment', 'temporal constraint', 'interaction protocol', 'logic and formal model of agency and multi-agent system', 'agent communication language and protocol', 'predictability level', 'pre-commitment', 'conditional commitment', 'interactive behavior', 'causal relationship', 'emergent protocol', 'level of predictability', 'linear implication']\n++++++ model: ['commitments', 'protocol actions', 'temporal linear logic', 'reason', 'open system such', 'agents', 'scenarios such', 'mechanism', 'resource-sensitive reasoning', 'interactive behaviors', 'tight integration', 'protocols actions', 'temporal', 'online commerce', 'protocols']\n****  75  --->  0   7.999999999999999   6.666666666666667\nWhen designing a mechanism there are several desirable properties to maintain such as incentive compatibility (IC) , individual rationality (IR) , and budget balance (BB) . It is well known [15] that it is impossible for a mechanism to maximize social welfare whilst also being IR , IC , and BB . There have been several attempts to circumvent [15] by trading welfare for BB , e.g. , in domains such as double-sided auctions[13] , distributed markets[3] and supply chain problems[2 , 4] . In this paper we provide a procedure called a Generalized Trade Reduction (GTR) for single-value players , which given an IR and IC mechanism , outputs a mechanism which is IR , IC and BB with a loss of welfare . We bound the welfare achieved by our procedure for a wide range of domains . In particular , our results improve on existing solutions for problems such as double sided markets with homogenous goods , distributed markets and several kinds of supply chains . Furthermore , our solution provides budget balanced mechanisms for several open problems such as combinatorial double-sided auctions and distributed markets with strategic transportation edges.\n++++++ label:  ['budget-balanced mechanism', 'inequality in welfare', 'external competition', 'homogeneous good', 'multi-minded player', 'efficiency', 'generalized trade reduction', 'player power', 'internal competition', 'optimality', 'budget balance', 'gtr', 'power of player', 'spatially distributed market', 'trade reduction']\n++++++ model: ['ic', 'ir', 'double-sided auctions', 'several open problems such', ']', 'bb', '[', 'problems such', 'mechanism', 'budget balance', 'combinatorial double-sided auctions', 'welfare', 'such', 'strategic transportation edges', 'supply chain problems']\n****  76  --->  0   0   0\nOnline reviews have become increasingly popular as a way to judge the quality of various products and services . Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult . In this paper , we investigate underlying factors that influence user behavior when reporting feedback . We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review , and patterns in the time sequence of reports . We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature . Second , we show that a user\"s rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews . Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias . Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.\n++++++ label:  ['utility of the product', 'the product utility', 'semantic orientation of product evaluation', 'brag-and-moan model', 'great probability bi-modal', 'clear incentive absence', 'rating', 'u-shaped distribution', 'correlation', 'reputation mechanism', 'large span of time', 'feature-by-feature estimator of quality', 'absence of clear incentive', 'online review']\n++++++ model: ['user', 'review', 'user bias', 'quality', 'tripadvisor website', 'hotel reviews', 'various products', 'user biases', 'certain feature', 'feature', 'online reviews', 'statistical evidence', 'true quality', 's rating', 'way']\n****  77  --->  0   0   6.25\nIn a wide range of markets , individual buyers and sellers often trade through intermediaries , who determine prices via strategic considerations . Typically , not all buyers and sellers have access to the same intermediaries , and they trade at correspondingly different prices that reflect their relative amounts of power in the market . We model this phenomenon using a game in which buyers , sellers , and traders engage in trade on a graph that represents the access each buyer and seller has to the traders . In this model , traders set prices strategically , and then buyers and sellers react to the prices they are offered . We show that the resulting game always has a subgame perfect Nash equilibrium , and that all equilibria lead to an efficient (i.e . socially optimal) allocation of goods . We extend these results to a more general type of matching market , such as one finds in the matching of job applicants and employers . Finally , we consider how the profits obtained by the traders depend on the underlying graph - roughly , a trader can command a positive profit if and only if it has an essential connection in the network structure , thus providing a graph-theoretic basis for quantifying the amount of competition among traders . Our work differs from recent studies of how price is affected by network structure through our modeling of price-setting as a strategic activity carried out by a subset of agents in the system , rather than studying prices set via competitive equilibrium or by a truthful mechanism.\n++++++ label:  ['initial endowment of money', 'bid price', 'economics and finance', 'maximum and minimum amount', 'market', 'perfect competition', 'algorithmic game theory', 'interaction of buyer and seller', 'trader strategic behavior', 'benefit', 'strategic behavior of trader', 'monopoly', 'trade network', 'buyer and seller interaction', 'trading network', 'money initial endowment', 'complementary slackness']\n++++++ model: ['price', 'trade', 'buyer', 'trader', 'seller', 'traders', 'prices', 'subgame perfect nash equilibrium', 'sellers', 'buyers', 'network structure', 'market', 'model', 'game', 'graph']\n****  78  --->  21.052631578947363   16.666666666666664   20.689655172413794\n****  79  --->  0   17.391304347826086   21.42857142857143\nThis paper addresses the problem of fair equilibrium selection in graphical games . Our approach is based on the data structure called the best response policy , which was proposed by Kearns et al . [13] as a way to represent all Nash equilibria of a graphical game . In [9] , it was shown that the best response policy has polynomial size as long as the underlying graph is a path . In this paper , we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants . Another attractive solution concept is a Nash equilibrium that maximizes the social welfare . We show that , while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree) , there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size . These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.\n++++++ label:  ['strategy profile', 'nash equilibrium', 'approximation', 'exponential-time algorithm', 'approximation scheme', 'overall payoff', 'several drawback', 'social welfare', 'distributing profit', 'graphical game', 'degree-bounded graph', 'various sociallydesirable property', 'integer-payoff graphical game g']\n++++++ model: ['best response policy', 'polynomial size', 'graph', 'equilibrium', 'nash equilibria', 'nash equilibrium', 'various fairness criteria', 'graphical game', 'fair equilibrium selection', 'attractive solution concept', 'algebraic numbers', 'paper', 'problem', 'graphical games', 'social welfare']\n****  80  --->  14.285714285714286   21.05263157894737   24.999999999999996\n****  81  --->  11.11111111111111   17.391304347826086   14.285714285714288\n****  82  --->  22.22222222222222   26.086956521739133   28.571428571428577\n****  83  --->  11.11111111111111   8.695652173913043   7.142857142857144\n****  84  --->  10.526315789473681   16.666666666666664   13.793103448275861\n****  85  --->  22.22222222222222   17.391304347826086   21.42857142857143\n****  86  --->  0   0   6.666666666666667\nWe consider a permutation betting scenario , where people wager on the final ordering of n candidates: for example , the outcome of a horse race . We examine the auctioneer problem of risklessly matching up wagers or , equivalently , finding arbitrage opportunities among the proposed wagers . Requiring bidders to explicitly list the orderings that they\"d like to bet on is both unnatural and intractable , because the number of orderings is n! and the number of subsets of orderings is 2n!  . We propose two expressive betting languages that seem natural for bidders , and examine the computational complexity of the auctioneer problem in each case . Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering , for example , horse A will finish in positions 4 , 9 , or 13-21 , or that a position will be taken by some subset of candidates , for example horse A , B , or D will finish in position 2 . For subset betting , we show that the auctioneer problem can be solved in polynomial time if orders are divisible . Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate , for example horse A will beat horse B . We prove that the auctioneer problem becomes NP-hard for pair betting . We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time . We also show that a natural greedy algorithm gives a poor approximation for indivisible orders.\n++++++ label:  ['pair-betting market', 'minimum feedback', 'permutation combinatoric', 'greedy algorithm', 'order match', 'complex polynomial transformation', 'expressive bet', 'permutation betting', 'prediction market', 'subset betting', 'computational complexity', 'information aggregation', 'polynomial-time algorithm', 'bilateral trading partner', 'bipartite graph']\n++++++ model: ['n', 'b', 'd', 'auctioneer problem', 'subset', 'horse', 'pair betting', 'candidate', 'position', 'example', 'example horse', 'polynomial time', 'subset betting', 'final ordering', 'orderings']\n****  87  --->  11.76470588235294   9.09090909090909   7.407407407407408\n****  88  --->  0   0   0\nWe develop a framework for trading in compound securities: financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic . Buying or selling securities-which can be thought of as betting on or against a particular future outcome-allows agents both to hedge risk and to profit (in expectation) on subjective predictions . A compound securities market allows agents to place bets on arbitrary boolean combinations of events , enabling them to more closely achieve their optimal risk exposure , and enabling the market as a whole to more closely achieve the social optimum . The tradeoff for allowing such expressivity is in the complexity of the agents\" and auctioneer\"s optimization problems . We develop and motivate the concept of a compound securities market , presenting the framework through a series of formal definitions and examples . We then analyze in detail the auctioneer\"s matching problem . We show that , with n events , the matching problem is co-NP-complete in the divisible case and p 2-complete in the indivisible case . We show that the latter hardness result holds even under severe language restrictions on bids . With log n events , the problem is polynomial in the divisible case and NP-complete in the indivisible case . We briefly discuss matching algorithms and tractable special cases.\n++++++ label:  ['combinatorial betting', 'gamble', 'speculate', 'bayesian network', 'combined-value trading', 'arbitrary logical combination', 'compound security market', 'approximation algorithm', 'effective probability assessment', 'base security', 'combinatorial bet', 'trade in financial instrument base on logical formula', 'risk allocation', 'payoff vector', 'tractable case', 'hedge', 'bet', 'information aggregation', 'compound security', 'computational complexity of match']\n++++++ model: ['s', 'divisible case', 'compound securities market', 'compound securities', 'matching problem', 'problem', 'n events', 'log n events', 'particular future outcome-allows agents', 'latter hardness result', 'indivisible case', 's optimization problems', 'market', 'np-complete', 'tractable special cases']\n****  89  --->  0   0   0\nMuch recent research concerns systems , such as the Internet , whose components are owned and operated by different parties , each with his own selfish goal . The field of Algorithmic Mechanism Design handles the issue of private information held by the different parties in such computational settings . This paper deals with a complementary problem in such settings: handling the hidden actions that are performed by the different parties . Our model is a combinatorial variant of the classical principalagent problem from economic theory . In our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf , but their actions are hidden from him . Our focus is on cases where complex combinations of the efforts of the agents influence the outcome . The principal motivates the agents by offering to them a set of contracts , which together put the agents in an equilibrium point of the induced game . We present formal models for this setting , suggest and embark on an analysis of some basic issues , but leave many questions open.\n++++++ label:  ['combinatorial agency', 'nash equilibrium', 'contractible action', 'k-orbit', 'price of unaccountability', 'unaccountability price', 'con', 'quality of service', 'classical principalagent', 'principal-agent model', 'agency theory', 'series-parallel network', 'service quality', 'anonymous technology', 'optimal set of contract', 'incentive', 'contract optimal set']\n++++++ model: ['set', 'much recent research concerns systems', 'different parties', 'algorithmic mechanism design', 'principal', 'setting', 'agents', 'present formal models', 'classical principalagent problem', 'such', 'own selfish goal', 'such computational settings', 'many questions', 'model', 'basic issues']\n****  90  --->  0   0   6.896551724137931\nA sequence of prices and demands are rationalizable if there exists a concave , continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price . Afriat [1] presented necessary and sufficient conditions for a finite sequence to be rationalizable . Varian [20] and later Blundell et al . [3 , 4] continued this line of work studying nonparametric methods to forecasts demand . Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast . The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts . Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable , but that there exist interesting and potentially useful classes that are learnable from finite samples . We also present a learning algorithm that is an adaptation of a new proof of Afriat\"s theorem due to Teo and Vohra [17].\n++++++ label:  ['machine learn', 'finite set of observation', 'complexity problem', 'rationalizability', 'reveal preference', 'income-lipschitz', 'learning from revealed preference', 'fat shattering dimension', 'monotone concave utility function', 'fat shatter', 'probably approximately correct', 'observation finite set', 'forecast', 'demand function']\n++++++ model: ['al', 'demand functions', 'able', '[', 'class', ']', 'monotone utility function such', 'utility function', 'present paper complements', 's theorem due', 'forecast', 'afriat', 'classes', 'rationalizable', 'complexity']\n****  91  --->  0   8.695652173913043   7.142857142857144\nWe present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem . The bidding language in our auctions allows marginal-decreasing piecewise constant curves . First , we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem , which computes a (1 + )approximation in worst-case time T = O(n3 / ) , given n bids each with a constant number of pieces . Second , we embed this approximation scheme within a Vickrey-Clarke-Groves (VCG) mechanism and compute payments to n agents for an asymptotic cost of O(T log n) . The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by /(1+ )V  , where V is the total surplus in the efficient outcome.\n++++++ label:  ['reverse auction', 'bidding language', 'forward auction', 'strategyproof', 'approximation algorithm', 'single-good multi-unit allocation problem', 'multi-unit auction', 'equilibrium', 'approximately-efficient and approximately strategyproof auction mechanism', 'marginal-decreasing piecewise constant curve', 'dynamic programming', 'vickrey-clarke-grove', 'fully polynomial-time approximation scheme']\n++++++ model: ['o', 'v', 'multi-unit allocation problem', 'worst-case time t = o', 'approximation scheme', 'single-good multi-unit allocation problem', 'marginal-decreasing piecewise constant curves', 'approximatelystrategyproof auction mechanism', 't log n', 'approximation', 'polynomial-time approximation scheme', 'maximal possible gain', 'n bids', 'total surplus', 'constant number']\n****  92  --->  0   0   6.666666666666667\nInternet search companies sell advertisement slots based on users\" search queries via an auction . While there has been previous work on the auction process and its game-theoretic aspects , most of it focuses on the Internet company . In this work , we focus on the advertisers , who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget . We model the entire process and study this budget optimization problem . While most variants are NP-hard , we show , perhaps surprisingly , that simply randomizing between two uniform strategies that bid equally on all the keywords works well . More precisely , this strategy gets at least a 1  1/e fraction of the maximum clicks possible . As our preliminary experiments show , such uniform strategies are likely to be practical . We also present inapproximability results , and optimal algorithms for variants of the budget optimization problem.\n++++++ label:  ['bid', 'game theory', 'internet', 'keyword', 'search-based advertising auction', 'advertiser', 'lp', 'optimization', 'budget optimization', 'auction', 'generalized second price', 'vickrey clark grove', 'uniform bidding strategy', 'intriguing heuristic', 'sponsor search']\n++++++ model: ['budget optimization problem', 'budget', 'work', 'uniform strategies', 'such uniform strategies', 'present inapproximability results', 'internet search companies', 'complex optimization problem', 'maximum clicks possible', 'keywords', 'game-theoretic aspects', 'variants', 'auction', 'search queries', 'user clicks']\n****  93  --->  0   8.333333333333332   6.896551724137931\nWhile traditional mechanism design typically assumes isomorphism between the agents\" type- and action spaces , in many situations the agents face strict restrictions on their action space due to , e.g. , technical , behavioral or regulatory reasons . We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces . Our contribution is threefold . First , we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies , and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost . Second , we identify necessary conditions for the optimality of action-bounded mechanisms , and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives . Finally , we prove that for any multilinear social-choice rule , the optimal mechanism with k actions incurs an expected loss of O( 1 k2 ) compared to the optimal mechanisms with unrestricted action spaces . Our results apply to various economic and computational settings , and we demonstrate their applicability to signaling games , public-good models and routing in networks.\n++++++ label:  ['implementation', 'multilinear function', 'single-crossing condition', 'communication complexity', 'probability of success', 'single-cross condition', 'action-bounded mechanism', 'social-choice function', 'bounded action space', 'decision function', 'dominant strategy', 'optimal mechanism', 'mechansm design', 'success probability']\n++++++ model: ['o', 'multilinear social-choice rule', 'restricted action spaces', 'action spaces', 'information-theoretically optimal social-choice rule', 'optimal mechanism', 'unrestricted action spaces', 'optimal mechanisms', 'mechanism design', 'action space due', 'traditional mechanism design', 'single-parameter environments', 'action-bounded mechanisms', 'k actions', 'strategies']\n****  94  --->  10.0   15.999999999999998   13.333333333333334\n****  95  --->  12.500000000000002   19.047619047619047   15.384615384615383\n****  96  --->  0   0   0\nKeyword auctions lie at the core of the business models of today\"s leading search engines . Advertisers bid for placement alongside search results , and are charged for clicks on their ads . Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates . We consider a family of ranking rules that contains those typically used to model Yahoo! and Google\"s auction designs as special cases . We find that in general neither of these is necessarily revenue-optimal in equilibrium , and that the choice of ranking rule can be guided by considering the correlation between bidders\" values and click-through rates . We propose a simple approach to determine a revenue-optimal ranking rule within our family , taking into account effects on advertiser satisfaction and user experience . We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword.\n++++++ label:  ['profit', 'revenue-optimal ranking', 'rank-by-revenue', 'ranking rule', 'optimal auction design problem', 'rank-by-bid', 'revenue', 'advertising revenue', 'advertisement', 'pricing search keyword', 'keyword auction', 'search engine', 'sponsor search', 'sponsored search']\n++++++ model: ['s', 'bid', 'rule', 'click-through rate data', 'revenue-optimal ranking rule', 'click-through rates', 'yahoo', 's auction designs', 'potential clickthrough rates', 'monte-carlo simulations', 'revenue-optimal', 'general neither', 'high-volume keyword', 'keyword auctions', 'core']\n****  97  --->  0   0   0\nIn many settings , competing technologies - for example , operating systems , instant messenger systems , or document formatscan be seen adopting a limited amount of compatibility with one another; in other words , the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability . There are a range of reasons why this phenomenon occurs , many of which - based on legal , social , or business considerations - seem to defy concise mathematical models . Despite this , we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks , thus offering a basic explanation for this phenomenon in purely strategic terms . Our approach builds on work on the diffusion of innovations in the economics literature , which seeks to model how a new technology A might spread through a social network of individuals who are currently users of technology B . We consider several ways of capturing the compatibility of A and B , focusing primarily on a model in which users can choose to adopt A , adopt B , or - at an extra cost - adopt both A and B . We characterize how the ability of A to spread depends on both its quality relative to B , and also this additional cost of adopting both , and find some surprising non-monotonicity properties in the dependence on these parameters: in some cases , for one technology to survive the introduction of another , the cost of adopting both technologies must be balanced within a narrow , intermediate range . We also extend the framework to the case of multiple technologies , where we find that a simple This work has been supported in part by NSF grants CCF0325453 , IIS-0329064 , CNS-0403340 , and BCS-0537606 , a Google Research Grant , a Yahoo! Research Alliance Grant , the Institute for the Social Sciences at Cornell , and the John D . and Catherine T . MacArthur Foundation . model captures the phenomenon of two firms adopting a limited strategic alliance to defend against a new , third technology.\n++++++ label:  ['potential function', 'interoperability', 'diffusion process', 'diffusion of innovation', 'contagion on network', 'game-theoretic diffusion model', 'limited compatibility', 'algorithmic game theory', 'non-convexity property', \"morris's theorem\", 'bilinguality', 'contagion threshold', 'contagion game', 'strategic incompatibility', 'innovation diffusion', 'characterization']\n++++++ model: ['b', 'work', 'model', 'social network', 'adopt b', 'technology', 'social', 'technologies', 'multiple technologies', 'google research grant', 'instant messenger systems', 'surprising non-monotonicity properties', 'research alliance grant', 'cost', 'compatibility']\n****  98  --->  7.142857142857142   6.0606060606060606   15.789473684210527\n****  99  --->  0   10.256410256410257   13.636363636363637\nAccording to economic theory-supported by empirical and laboratory evidence-the equilibrium price of a financial security reflects all of the information regarding the security\"s value . We investigate the computational process on the path toward equilibrium , where information distributed among traders is revealed step-by-step over time and incorporated into the market price . We develop a simplified model of an information market , along with trading strategies , in order to formalize the computational properties of the process . We show that securities whose payoffs cannot be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory . On the other hand , securities whose payoffs are threshold functions are guaranteed to converge , for all prior probability distributions . Moreover , these threshold securities converge in at most n rounds , where n is the number of bits of distributed information . We also prove a lower bound , showing a type of threshold security that requires at least n/2 rounds to converge in the worst case.\n++++++ label:  ['worst case', 'number of bit', 'distributed information', 'round', 'information market', 'computational property of the process', 'simplified model', 'lower bound', 'financial security', 'convergence to equilibrium', 'market price', 'economic theory', 'trader', 'empirical and laboratory evidence', \"security's value\", 'trading strategy', 'efficient market hypothesis', 'probability distribution', 'information aggregation', 'bit number', 'threshold function', 'path toward equilibrium', 'computational process', 'payoff', 'distribute information market', 'rational expectation', 'equilibrium price', 'market computation', 'security']\n++++++ model: ['n', 'laboratory evidence-the equilibrium price', 'threshold functions', 'least n/2 rounds', 'information', 'economic theory', 'prior probability distributions', 'most n rounds', 'weighted threshold functions', 'security', 'equilibrium', 'threshold securities converge', 'securities', 'economic theory-supported', 'information market']\n---> f1@k_score:  {'dataset': 'semeval2010', 'top_k': 5, 'average_score': 7.5267391575703995}\n---> f1@k_score:  {'dataset': 'semeval2010', 'top_k': 10, 'average_score': 9.75747685074564}\n---> f1@k_score:  {'dataset': 'semeval2010', 'top_k': 15, 'average_score': 10.889482371446915}\n","output_type":"stream"}],"execution_count":6}]}