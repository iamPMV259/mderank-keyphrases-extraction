{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11114107,"sourceType":"datasetVersion","datasetId":6929593}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary packages (run these cells if not already installed)\n!pip install stanfordcorenlp==3.9.1.1\n!pip install torchvision\n\n# Imports\nimport re\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel, T5Tokenizer\nimport nltk\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset\nfrom stanfordcorenlp import StanfordCoreNLP\nfrom tqdm import tqdm\nimport json\nimport os\nimport sys\nimport codecs\nimport asyncio\nimport aiofiles\n\nfrom nltk.stem import PorterStemmer\nfrom scipy.stats import zscore","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T18:01:23.985518Z","iopub.execute_input":"2025-03-28T18:01:23.985944Z","iopub.status.idle":"2025-03-28T18:01:47.777412Z","shell.execute_reply.started":"2025-03-28T18:01:23.985871Z","shell.execute_reply":"2025-03-28T18:01:47.776360Z"}},"outputs":[{"name":"stdout","text":"Collecting stanfordcorenlp==3.9.1.1\n  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp==3.9.1.1) (5.9.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp==3.9.1.1) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp==3.9.1.1) (2025.1.31)\nDownloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\nInstalling collected packages: stanfordcorenlp\nSuccessfully installed stanfordcorenlp-3.9.1.1\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Check working directory (optional)\n!pwd\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nclass MDERank:\n    def __init__(self, model_name=\"bert-base-uncased\", pooling=\"max\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.pooling = pooling\n\n    def compute_embedding(self, text):\n        # Chuẩn hóa đầu vào và lấy output từ mô hình BERT\n        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Lấy hidden_states: [batch_size, sequence_length, hidden_size]\n        hidden_states = outputs.last_hidden_state[0]  # (seq_len, hidden_size)\n        if self.pooling == \"max\":\n            embedding, _ = torch.max(hidden_states, dim=0)\n        elif self.pooling == \"avg\":\n            embedding = torch.mean(hidden_states, dim=0)\n        else:\n            embedding = torch.mean(hidden_states, dim=0)\n        return embedding.numpy()\n\n    def extract_candidates(self, text):\n        \"\"\"\n        Sử dụng NLTK để tách từ, gán nhãn POS và trích xuất các cụm từ ứng viên\n        theo pattern: liên tục các từ có tag bắt đầu bằng JJ (tính từ) hoặc NN (danh từ).\n        \"\"\"\n        tokens = word_tokenize(text)\n        tagged = pos_tag(tokens)\n        candidates = []\n        candidate = []\n        for word, tag in tagged:\n            if tag.startswith(\"JJ\") or tag.startswith(\"NN\"):\n                candidate.append(word)\n            else:\n                if candidate:\n                    phrase = \" \".join(candidate)\n                    candidates.append(phrase)\n                    candidate = []\n        if candidate:\n            phrase = \" \".join(candidate)\n            candidates.append(phrase)\n        # Loại bỏ các cụm từ trùng lặp và có độ dài ít nhất 1 từ\n        candidates = list(set([c for c in candidates if len(c.split()) >= 1]))\n        return candidates\n\n    def mask_text(self, text, candidate):\n        \"\"\"\n        Thay thế các xuất hiện của candidate trong text bằng [MASK] với số lượng token tương ứng.\n        \"\"\"\n        candidate_tokens = candidate.split()\n        mask_token = \" \".join([\"[MASK]\"] * len(candidate_tokens))\n        # Sử dụng regex để thay thế, không phân biệt hoa thường\n        pattern = re.compile(re.escape(candidate), re.IGNORECASE)\n        masked_text = pattern.sub(mask_token, text)\n        return masked_text\n\n    def cosine_similarity(self, vec1, vec2):\n        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n\n    def rank_keyphrases(self, text):\n        \"\"\"\n        Tính toán embedding của văn bản gốc và đối với mỗi ứng viên, tính embedding của văn bản đã mask.\n        Sau đó, tính cosine similarity giữa hai embedding này. Ứng viên có similarity thấp hơn (nghĩa là mất thông tin lớn)\n        được xem là quan trọng hơn.\n        \"\"\"\n        original_embedding = self.compute_embedding(text)\n        candidates = self.extract_candidates(text)\n        scores = {}\n        for candidate in candidates:\n            masked_text = self.mask_text(text, candidate)\n            masked_embedding = self.compute_embedding(masked_text)\n            sim = self.cosine_similarity(original_embedding, masked_embedding)\n            scores[candidate] = sim\n        # Sắp xếp các ứng viên theo thứ tự tăng dần của similarity\n\n        results = [(k, v) for k, v in scores.items()]\n        keyphrases = [kw[0] for kw in results]\n        scores = [kw[1] for kw in results]\n        z_score = zscore(scores)\n        selected_keyphrases = [(kw, z) for kw, z in zip(keyphrases, z_score) if z <= 0.5]\n\n        # Sắp xếp các ứng viên theo cosine similarity tăng dần (ứng viên có mất thông tin lớn hơn có giá trị similarity thấp hơn)\n        ranked = sorted(selected_keyphrases, key=lambda x: x[1])\n        return ranked\n\n\nclass AsyncMDERank:\n    def __init__(self, model_name=\"bert-base-uncased\", pooling=\"max\"):\n        \"\"\"\n        Khởi tạo lớp AsyncMDERank với model BERT và phương pháp pooling.\n\n        Args:\n            model_name (str): Tên của model BERT.\n            pooling (str): Phương pháp pooling, mặc định là \"max\". Các tùy chọn khác có thể là \"avg\".\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.pooling = pooling\n\n    async def compute_embedding(self, text):\n        \"\"\"\n        Tính toán embedding của văn bản sử dụng model BERT một cách bất đồng bộ.\n\n        Args:\n            text (str): Văn bản đầu vào.\n\n        Returns:\n            numpy.ndarray: Embedding của văn bản dưới dạng mảng numpy.\n        \"\"\"\n        def _compute():\n            # Chuẩn hóa đầu vào và lấy output từ model BERT\n            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Lấy hidden_states: [seq_len, hidden_size]\n            hidden_states = outputs.last_hidden_state[0]\n            if self.pooling == \"max\":\n                embedding, _ = torch.max(hidden_states, dim=0)\n            elif self.pooling == \"avg\":\n                embedding = torch.mean(hidden_states, dim=0)\n            else:\n                embedding = torch.mean(hidden_states, dim=0)\n            return embedding.numpy()\n        \n        return await asyncio.to_thread(_compute)\n\n    async def extract_candidates(self, text):\n        \"\"\"\n        Sử dụng NLTK để tách từ, gán nhãn POS và trích xuất các cụm từ ứng viên dựa theo pattern:\n        các từ liên tiếp có tag bắt đầu bằng 'JJ' (tính từ) hoặc 'NN' (danh từ).\n\n        Args:\n            text (str): Văn bản đầu vào.\n\n        Returns:\n            list[str]: Danh sách các cụm từ ứng viên (đã loại bỏ trùng lặp).\n        \"\"\"\n        def _extract():\n            tokens = word_tokenize(text)\n            tagged = pos_tag(tokens)\n            candidates = []\n            candidate = []\n            for word, tag in tagged:\n                if tag.startswith(\"JJ\") or tag.startswith(\"NN\"):\n                    candidate.append(word)\n                else:\n                    if candidate:\n                        phrase = \" \".join(candidate)\n                        candidates.append(phrase)\n                        candidate = []\n            if candidate:\n                phrase = \" \".join(candidate)\n                candidates.append(phrase)\n            # Loại bỏ các cụm từ trùng lặp và chỉ giữ lại cụm có ít nhất 1 từ\n            candidates = list(set([c for c in candidates if len(c.split()) >= 1]))\n            return candidates\n        \n        return await asyncio.to_thread(_extract)\n\n    def mask_text(self, text, candidate):\n        \"\"\"\n        Thay thế tất cả các xuất hiện của candidate trong text bằng [MASK] với số lượng token tương ứng.\n\n        Args:\n            text (str): Văn bản gốc.\n            candidate (str): Cụm từ cần mask.\n\n        Returns:\n            str: Văn bản sau khi đã mask.\n        \"\"\"\n        candidate_tokens = candidate.split()\n        mask_token = \" \".join([\"[MASK]\"] * len(candidate_tokens))\n        pattern = re.compile(re.escape(candidate), re.IGNORECASE)\n        masked_text = pattern.sub(mask_token, text)\n        return masked_text\n\n    def cosine_similarity(self, vec1, vec2):\n        \"\"\"\n        Tính cosine similarity giữa hai vector.\n\n        Args:\n            vec1 (numpy.ndarray): Vector thứ nhất.\n            vec2 (numpy.ndarray): Vector thứ hai.\n\n        Returns:\n            float: Giá trị cosine similarity.\n        \"\"\"\n        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n\n    async def rank_keyphrases(self, text):\n        \"\"\"\n        Tính toán embedding của văn bản gốc và đối với mỗi ứng viên, tính embedding của văn bản đã mask.\n        Sau đó, tính cosine similarity giữa hai embedding này. Các ứng viên có similarity thấp hơn (nghĩa là mất\n        thông tin lớn) được xem là quan trọng hơn.\n\n        Args:\n            text (str): Văn bản đầu vào.\n\n        Returns:\n            list[tuple[str, float]]: Danh sách các cặp (ứng viên, similarity) được sắp xếp theo thứ tự tăng dần của similarity.\n        \"\"\"\n        # Tính embedding cho văn bản gốc và trích xuất các ứng viên bất đồng bộ\n        original_embedding = await self.compute_embedding(text)\n        candidates = await self.extract_candidates(text)\n\n        async def process_candidate(candidate):\n            masked_text = self.mask_text(text, candidate)\n            masked_embedding = await self.compute_embedding(masked_text)\n            sim = self.cosine_similarity(original_embedding, masked_embedding)\n            return candidate, sim\n\n        # Chạy đồng thời tính toán cho tất cả các ứng viên\n        tasks = [process_candidate(candidate) for candidate in candidates]\n        results = await asyncio.gather(*tasks)\n        keyphrases = [kw[0] for kw in results]\n        scores = [kw[1] for kw in results]\n        z_score = zscore(scores)\n        selected_keyphrases = [(kw, z) for kw, z in zip(keyphrases, z_score) if z <= 0.5]\n\n        # Sắp xếp các ứng viên theo cosine similarity tăng dần (ứng viên có mất thông tin lớn hơn có giá trị similarity thấp hơn)\n        ranked = sorted(selected_keyphrases, key=lambda x: x[1])\n\n        return ranked","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T18:01:47.778951Z","iopub.execute_input":"2025-03-28T18:01:47.779536Z","iopub.status.idle":"2025-03-28T18:01:48.138697Z","shell.execute_reply.started":"2025-03-28T18:01:47.779503Z","shell.execute_reply":"2025-03-28T18:01:48.137438Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"async def extract_keyphrases(text, top_k=50):\n    \"\"\"\n    Hàm bọc để trích xuất keyphrase từ văn bản.\n    Trả về danh sách top_k keyphrase có score thấp nhất (nghĩa là quan trọng nhất).\n    \"\"\"\n    mde = AsyncMDERank()\n    text = text.lower()\n    ranked = await mde.rank_keyphrases(text)\n    top_candidates = [phrase for phrase, score in ranked[:top_k]]\n    return top_candidates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T18:01:48.140534Z","iopub.execute_input":"2025-03-28T18:01:48.140932Z","iopub.status.idle":"2025-03-28T18:01:48.147235Z","shell.execute_reply.started":"2025-03-28T18:01:48.140878Z","shell.execute_reply":"2025-03-28T18:01:48.145941Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def clean_labels(labels):\n    clean_labels = {}\n    for id in labels:\n        label = labels[id]\n        clean_label = []\n        for kp in label:\n            if kp.find(\";\") != -1:\n                left, right = kp.split(\";\")\n                clean_label.append(left)\n                clean_label.append(right)\n            else:\n                clean_label.append(kp)\n        clean_labels[id] = clean_label        \n    return clean_labels\n\n# --------------------\n# Các hàm đọc file bất đồng bộ\n# --------------------\n\nasync def get_long_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/nus/nus_test.json\"):\n    \"\"\" Load file.jsonl bất đồng bộ \"\"\"\n    data = {}\n    labels = {}\n    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:\n        json_text = await f.readlines()\n        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n            try:\n                jsonl = json.loads(line)\n                keywords = jsonl['keywords'].lower().split(\";\")\n                abstract = jsonl['abstract']\n                doc = abstract\n                doc = re.sub(r'\\. ', ' . ', doc)\n                doc = re.sub(r', ', ' , ', doc)\n                doc = doc.replace('\\n', ' ')\n                data[jsonl['name']] = doc\n                labels[jsonl['name']] = keywords\n            except Exception as e:\n                raise ValueError(f\"Lỗi xử lý dòng {i}: {e}\")\n    labels = clean_labels(labels)\n    return data, labels\n\nasync def get_duc2001_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/DUC2001\"):\n    pattern = re.compile(r'<TEXT>(.*?)</TEXT>', re.S)\n    data = {}\n    labels = {}\n    for dirname, _, filenames in os.walk(file_path):\n        for fname in filenames:\n            if fname == \"annotations.txt\":\n                infile = os.path.join(dirname, fname)\n                async with aiofiles.open(infile, 'rb') as f:\n                    text = await f.read()\n                    text = text.decode('utf8')\n                lines = text.splitlines()\n                for line in lines:\n                    left, right = line.split(\"@\")\n                    d = right.split(\";\")[:-1]\n                    l = left\n                    labels[l] = d\n            else:\n                infile = os.path.join(dirname, fname)\n                async with aiofiles.open(infile, 'rb') as f:\n                    text = await f.read()\n                    text = text.decode('utf8')\n                found = re.findall(pattern, text)\n                if found:\n                    data[fname] = found[0]\n    labels = clean_labels(labels)\n    return data, labels\n\nasync def get_inspec_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/Inspec\"):\n    data = {}\n    labels = {}\n    for dirname, _, filenames in os.walk(file_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            infile = os.path.join(dirname, fname)\n            if right == \"abstr\":\n                async with aiofiles.open(infile, 'r', encoding='utf-8') as f:\n                    text = await f.read()\n                    text = text.replace(\"%\", '')\n                    data[left] = text\n            elif right == \"uncontr\":\n                async with aiofiles.open(infile, 'r', encoding='utf-8') as f:\n                    text = await f.read()\n                    text = text.replace(\"\\n\\t\", ' ')\n                    text = text.replace(\"\\n\", ' ')\n                    label = text.split(\"; \")\n                    labels[left] = label\n    labels = clean_labels(labels)\n    return data, labels\n\nasync def get_semeval2017_data(data_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2017/docsutf8\", labels_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2017/keys\"):\n    data = {}\n    labels = {}\n    for dirname, _, filenames in os.walk(data_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            infile = os.path.join(dirname, fname)\n            async with aiofiles.open(infile, \"r\", encoding=\"utf-8\") as fi:\n                text = await fi.read()\n                text = text.replace(\"%\", '')\n            data[left] = text.lower()\n    for dirname, _, filenames in os.walk(labels_path):\n        for fname in filenames:\n            left, right = fname.split('.')\n            infile = os.path.join(dirname, fname)\n            async with aiofiles.open(infile, 'r', encoding='utf-8') as f:\n                text = await f.read()\n                text = text.strip()\n                ls = text.splitlines()\n                labels[left] = ls\n    labels = clean_labels(labels)\n    return data, labels\n\nasync def get_short_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/krapivin/kravipin_test.json\"):\n    data = {}\n    labels = {}\n    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:\n        json_text = await f.readlines()\n        for i, line in tqdm(enumerate(json_text), desc=\"Loading Doc ...\"):\n            try:\n                jsonl = json.loads(line)\n                keywords = jsonl['keywords'].lower().split(\";\")\n                abstract = jsonl['abstract']\n                doc = abstract\n                doc = re.sub(r'\\. ', ' . ', doc)\n                doc = re.sub(r', ', ' , ', doc)\n                doc = doc.replace('\\n', ' ')\n                doc = doc.replace('\\t', ' ')\n                data[i] = doc\n                labels[i] = keywords\n            except Exception as e:\n                raise ValueError(f\"Lỗi xử lý dòng {i}: {e}\")\n    labels = clean_labels(labels)\n    return data, labels\n\n# Các hàm đơn giản chỉ gọi hàm đã định nghĩa trên\nasync def get_krapivin_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/krapivin/krapivin_test.json\"):\n    return await get_short_data(file_path)\n\nasync def get_nus_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/nus/nus_test.json\"):\n    return await get_long_data(file_path)\n\nasync def get_semeval2010_data(file_path=\"/kaggle/input/keypharses-extraction-dataset/data/SemEval2010/semeval_test.json\"):\n    return await get_short_data(file_path)\n\nasync def get_dataset_data(dataset_name):\n    if dataset_name == \"duc2001\":\n        return await get_duc2001_data()\n    elif dataset_name == \"inspec\":\n        return await get_inspec_data()\n    elif dataset_name == \"krapivin\":\n        return await get_krapivin_data()\n    elif dataset_name == \"nus\":\n        return await get_nus_data()\n    elif dataset_name == \"semeval2010\":\n        return await get_semeval2010_data()\n    elif dataset_name == \"sameval2017\":\n        return await get_semeval2017_data()\n\n# Hàm tính F1 (không cần async vì chỉ tính toán)\ndef calculate_f1(keyphrases: list, ground_truth: list) -> float:\n    \"\"\"\n    Tính F1 score cho keyphrases dự đoán so với danh sách keyphrases thực tế.\n\n    Args:\n        keyphrases (list): Danh sách keyphrases dự đoán.\n        ground_truth (list): Danh sách keyphrases thực tế (labels[id]).\n\n    Returns:\n        float: F1 score dưới dạng phần trăm.\n    \"\"\"\n    # Tìm các keyphrase chung giữa dự đoán và thực tế\n    common = set(keyphrases) & set(ground_truth)\n    \n    # Tính precision và recall\n    precision = len(common) / len(keyphrases) if keyphrases else 0\n    recall = len(common) / len(ground_truth) if ground_truth else 0\n    \n    # Nếu không có giá trị nào thì F1 = 0\n    if precision + recall == 0:\n        return 0.0\n    \n    # Tính F1 score và chuyển đổi sang phần trăm\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1 * 100\n\n# Hàm chuyển cụm từ về dạng stem\ndef stem_phrase(phrase: str) -> str:\n    \"\"\"\n    Chuyển đổi một cụm từ sang dạng stem (dạng gốc).\n    Tách cụm từ thành từng từ, stem từng từ rồi nối lại theo thứ tự ban đầu.\n    \n    Args:\n        phrase (str): Cụm từ đầu vào.\n    \n    Returns:\n        str: Cụm từ sau stem.\n    \"\"\"\n    stemmer = PorterStemmer()\n    tokens = phrase.split()\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return \" \".join(stemmed_tokens)\n\n# Hàm loại bỏ trùng lặp (deduplication)\ndef dedup(input_list: list) -> list:\n    seen = set()\n    result = []\n    for item in input_list:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n\ndef dedup_stem(input_list: list) -> list:\n    return dedup([stem_phrase(item) for item in input_list])\n\n# Hàm ghi kết quả ra file JSON (bất đồng bộ)\ndef print_to_json(data_name, score):\n    \"\"\"\n    Ghi kết quả đánh giá ra file JSON.\n    \n    Parameters:\n      data_name (str): Tên dataset.\n      score (list): Danh sách kết quả đánh giá.\n    \"\"\"\n    average_score = sum(score) / len(score) if score else 0\n    result = {\n        \"dataset\": data_name,\n        \"average_score\": average_score,\n    }\n\n    # os.makedirs(\"data/results/\", exist_ok=True)\n    with open(f\"/kaggle/working/{data_name}.json\", \"w\") as outfile:\n        json.dump(result, outfile)\n\n# Hàm main bất đồng bộ\nasync def main():\n    # dataset = ['duc2001', 'inspec', 'krapivin', 'nus', 'semeval2010', 'sameval2017']\n    dataset = [ 'duc2001']\n    \n    for data_name in dataset:\n        data, labels = await get_dataset_data(data_name)\n        scores = []\n        cnt = 0\n        for id in data:\n            keyphrases = await extract_keyphrases(data[id])\n            labels[id] = dedup_stem(labels[id])\n            keyphrases = dedup_stem(keyphrases)\n            \n            print(f\"**{data_name}** \", id, \" : --> \", cnt, \" / \", len(data))\n            cnt += 1\n            # print(data[id])\n            # print(\"--- labels: \", labels[id])\n            # print(\"--- keyphrases: \", keyphrases)\n            score = calculate_f1(keyphrases, labels[id])\n            scores.append(score)\n            # print(\"F1 score:\", score)\n        print_to_json(data_name, scores)\n\nif __name__ == \"__main__\":\n    await main()\n    print(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T18:01:48.148315Z","iopub.execute_input":"2025-03-28T18:01:48.148624Z","iopub.status.idle":"2025-03-29T04:58:56.796813Z","shell.execute_reply.started":"2025-03-28T18:01:48.148597Z","shell.execute_reply":"2025-03-29T04:58:56.795465Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32cc9707c6d475587bee0d3d6519725"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52b49b6fe0547d28ad2cc6595bd8e47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd6f729038b4c11a5b931f69578dfe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b752c7dd1dbc44c3826d31a6c0e8d49e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"425b7e38f5234677a76ad8172f4eb735"}},"metadata":{}},{"name":"stdout","text":"**duc2001**  LA012090-0090  : -->  0  /  307\n**duc2001**  WSJ900720-0113  : -->  1  /  307\n**duc2001**  AP881009-0072  : -->  2  /  307\n**duc2001**  WSJ920103-0037  : -->  3  /  307\n**duc2001**  AP880913-0129  : -->  4  /  307\n**duc2001**  FT933-10881  : -->  5  /  307\n**duc2001**  AP901029-0035  : -->  8  /  307\n**duc2001**  AP880816-0234  : -->  9  /  307\n**duc2001**  AP891210-0079  : -->  10  /  307\n**duc2001**  AP880630-0295  : -->  11  /  307\n**duc2001**  AP900619-0006  : -->  12  /  307\n**duc2001**  WSJ900914-0127  : -->  13  /  307\n**duc2001**  AP901012-0032  : -->  14  /  307\n**duc2001**  FT932-12322  : -->  15  /  307\n**duc2001**  AP880705-0109  : -->  16  /  307\n**duc2001**  LA071590-0068  : -->  17  /  307\n**duc2001**  FT922-3171  : -->  18  /  307\n**duc2001**  FT911-2650  : -->  19  /  307\n**duc2001**  AP901203-0166  : -->  20  /  307\n**duc2001**  AP890117-0132  : -->  21  /  307\n**duc2001**  LA110590-0038  : -->  22  /  307\n**duc2001**  FT923-5835  : -->  23  /  307\n**duc2001**  AP891213-0004  : -->  24  /  307\n**duc2001**  SJMN91-06169114  : -->  25  /  307\n**duc2001**  AP890803-0008  : -->  26  /  307\n**duc2001**  AP880714-0142  : -->  27  /  307\n**duc2001**  WSJ910709-0115  : -->  28  /  307\n**duc2001**  WSJ870818-0002  : -->  29  /  307\n**duc2001**  AP890131-0280  : -->  31  /  307\n**duc2001**  FT934-12800  : -->  32  /  307\n**duc2001**  WSJ911224-0085  : -->  33  /  307\n**duc2001**  SJMN91-06193235  : -->  34  /  307\n**duc2001**  AP900829-0120  : -->  35  /  307\n**duc2001**  AP900428-0108  : -->  36  /  307\n**duc2001**  SJMN91-06084228  : -->  37  /  307\n**duc2001**  LA042790-0205  : -->  38  /  307\n**duc2001**  WSJ880621-0079  : -->  39  /  307\n**duc2001**  AP891201-0100  : -->  40  /  307\n**duc2001**  AP891116-0115  : -->  41  /  307\n**duc2001**  SJMN91-06312120  : -->  42  /  307\n**duc2001**  SJMN91-06192123  : -->  43  /  307\n**duc2001**  AP900316-0028  : -->  44  /  307\n**duc2001**  SJMN91-06129119  : -->  45  /  307\n**duc2001**  AP900703-0040  : -->  46  /  307\n**duc2001**  AP900601-0040  : -->  47  /  307\n**duc2001**  AP901130-0060  : -->  48  /  307\n**duc2001**  LA120290-0163  : -->  49  /  307\n**duc2001**  SJMN91-06105230  : -->  50  /  307\n**duc2001**  WSJ911106-0109  : -->  51  /  307\n**duc2001**  WSJ910702-0078  : -->  52  /  307\n**duc2001**  AP890511-0126  : -->  53  /  307\n**duc2001**  WSJ900705-0145  : -->  54  /  307\n**duc2001**  LA030889-0163  : -->  55  /  307\n**duc2001**  AP890111-0227  : -->  56  /  307\n**duc2001**  AP880914-0027  : -->  57  /  307\n**duc2001**  WSJ910710-0123  : -->  58  /  307\n**duc2001**  AP890714-0129  : -->  59  /  307\n**duc2001**  LA051590-0065  : -->  61  /  307\n**duc2001**  FT933-2760  : -->  62  /  307\n**duc2001**  FT931-341  : -->  63  /  307\n**duc2001**  AP900426-0054  : -->  64  /  307\n**duc2001**  FT942-11114  : -->  65  /  307\n**duc2001**  SJMN91-06246065  : -->  66  /  307\n**duc2001**  LA043089-0197  : -->  67  /  307\n**duc2001**  FT934-8628  : -->  68  /  307\n**duc2001**  LA121189-0017  : -->  69  /  307\n**duc2001**  AP890111-0217  : -->  70  /  307\n**duc2001**  FT922-10200  : -->  71  /  307\n**duc2001**  FBIS-41815  : -->  72  /  307\n**duc2001**  WSJ870123-0101  : -->  73  /  307\n**duc2001**  FT911-5176  : -->  74  /  307\n**duc2001**  AP900416-0188  : -->  75  /  307\n**duc2001**  FT943-4951  : -->  76  /  307\n**duc2001**  FBIS3-11919  : -->  77  /  307\n**duc2001**  LA061589-0143  : -->  78  /  307\n**duc2001**  AP901010-0036  : -->  79  /  307\n**duc2001**  LA081890-0039  : -->  80  /  307\n**duc2001**  AP880901-0052  : -->  81  /  307\n**duc2001**  WSJ870501-0141  : -->  82  /  307\n**duc2001**  FT923-6110  : -->  83  /  307\n**duc2001**  AP880926-0203  : -->  84  /  307\n**duc2001**  AP890313-0198  : -->  85  /  307\n**duc2001**  AP880705-0006  : -->  86  /  307\n**duc2001**  AP891017-0204  : -->  87  /  307\n**duc2001**  SJMN91-06191174  : -->  88  /  307\n**duc2001**  AP881222-0126  : -->  89  /  307\n**duc2001**  LA092790-0010  : -->  90  /  307\n**duc2001**  AP900419-0121  : -->  91  /  307\n**duc2001**  FT933-5709  : -->  92  /  307\n**duc2001**  FT941-1547  : -->  93  /  307\n**duc2001**  FBIS4-27602  : -->  94  /  307\n**duc2001**  SJMN91-06290146  : -->  95  /  307\n**duc2001**  AP880517-0226  : -->  96  /  307\n**duc2001**  LA100789-0007  : -->  97  /  307\n**duc2001**  LA032589-0044  : -->  98  /  307\n**duc2001**  LA042290-0104  : -->  99  /  307\n**duc2001**  WSJ880617-0024  : -->  100  /  307\n**duc2001**  AP880913-0204  : -->  101  /  307\n**duc2001**  WSJ910529-0003  : -->  102  /  307\n**duc2001**  AP890805-0126  : -->  103  /  307\n**duc2001**  AP880902-0062  : -->  104  /  307\n**duc2001**  AP880510-0178  : -->  105  /  307\n**duc2001**  FT923-6038  : -->  106  /  307\n**duc2001**  SJMN91-06143070  : -->  107  /  307\n**duc2001**  WSJ900918-0121  : -->  108  /  307\n**duc2001**  AP890802-0064  : -->  109  /  307\n**duc2001**  LA012590-0174  : -->  110  /  307\n**duc2001**  FT931-11394  : -->  111  /  307\n**duc2001**  AP901031-0024  : -->  112  /  307\n**duc2001**  FT922-8860  : -->  113  /  307\n**duc2001**  AP891116-0191  : -->  114  /  307\n**duc2001**  AP900721-0110  : -->  115  /  307\n**duc2001**  FT911-3463  : -->  116  /  307\n**duc2001**  LA040789-0051  : -->  117  /  307\n**duc2001**  AP830325-0143  : -->  118  /  307\n**duc2001**  LA021090-0005  : -->  119  /  307\n**duc2001**  AP890314-0237  : -->  120  /  307\n**duc2001**  AP890403-0123  : -->  122  /  307\n**duc2001**  WSJ911212-0080  : -->  123  /  307\n**duc2001**  WSJ910710-0148  : -->  124  /  307\n**duc2001**  AP880613-0161  : -->  125  /  307\n**duc2001**  AP890307-0150  : -->  126  /  307\n**duc2001**  LA081589-0043  : -->  127  /  307\n**duc2001**  SJMN91-06212161  : -->  128  /  307\n**duc2001**  FT923-5089  : -->  129  /  307\n**duc2001**  AP900512-0038  : -->  130  /  307\n**duc2001**  WSJ900615-0131  : -->  131  /  307\n**duc2001**  AP880928-0054  : -->  132  /  307\n**duc2001**  WSJ910628-0109  : -->  133  /  307\n**duc2001**  FT934-8748  : -->  134  /  307\n**duc2001**  AP891028-0022  : -->  135  /  307\n**duc2001**  SJMN91-06184021  : -->  136  /  307\n**duc2001**  AP881222-0119  : -->  137  /  307\n**duc2001**  AP880629-0159  : -->  138  /  307\n**duc2001**  LA081489-0025  : -->  139  /  307\n**duc2001**  AP880623-0135  : -->  140  /  307\n**duc2001**  AP900313-0191  : -->  141  /  307\n**duc2001**  AP890322-0010  : -->  142  /  307\n**duc2001**  FT934-10911  : -->  143  /  307\n**duc2001**  WSJ911030-0008  : -->  144  /  307\n**duc2001**  AP890502-0205  : -->  145  /  307\n**duc2001**  AP901013-0046  : -->  146  /  307\n**duc2001**  AP881126-0007  : -->  147  /  307\n**duc2001**  LA051190-0185  : -->  148  /  307\n**duc2001**  SJMN91-06184003  : -->  149  /  307\n**duc2001**  LA091889-0088  : -->  150  /  307\n**duc2001**  AP880914-0079  : -->  151  /  307\n**duc2001**  WSJ871215-0109  : -->  152  /  307\n**duc2001**  WSJ870306-0171  : -->  153  /  307\n**duc2001**  SJMN91-06142126  : -->  154  /  307\n**duc2001**  FT944-18184  : -->  155  /  307\n**duc2001**  LA103089-0070  : -->  156  /  307\n**duc2001**  AP890704-0043  : -->  157  /  307\n**duc2001**  LA052289-0050  : -->  158  /  307\n**duc2001**  AP890404-0260  : -->  159  /  307\n**duc2001**  AP901231-0012  : -->  160  /  307\n**duc2001**  AP880419-0131  : -->  161  /  307\n**duc2001**  AP900521-0063  : -->  162  /  307\n**duc2001**  LA041889-0039  : -->  163  /  307\n**duc2001**  LA120389-0130  : -->  164  /  307\n**duc2001**  FT934-11014  : -->  165  /  307\n**duc2001**  FT941-575  : -->  166  /  307\n**duc2001**  AP890930-0100  : -->  167  /  307\n**duc2001**  WSJ870227-0149  : -->  168  /  307\n**duc2001**  AP890907-0221  : -->  169  /  307\n**duc2001**  AP880331-0140  : -->  170  /  307\n**duc2001**  LA030489-0068  : -->  171  /  307\n**duc2001**  LA092189-0123  : -->  172  /  307\n**duc2001**  FT922-6646  : -->  173  /  307\n**duc2001**  LA042190-0060  : -->  174  /  307\n**duc2001**  WSJ920211-0036  : -->  175  /  307\n**duc2001**  FT943-5628  : -->  176  /  307\n**duc2001**  AP900607-0039  : -->  177  /  307\n**duc2001**  AP890325-0029  : -->  178  /  307\n**duc2001**  WSJ910718-0143  : -->  179  /  307\n**duc2001**  LA011889-0067  : -->  180  /  307\n**duc2001**  FBIS3-51875  : -->  181  /  307\n**duc2001**  LA071589-0076  : -->  182  /  307\n**duc2001**  AP890708-0135  : -->  183  /  307\n**duc2001**  FT921-9310  : -->  184  /  307\n**duc2001**  AP900217-0078  : -->  185  /  307\n**duc2001**  AP881018-0136  : -->  186  /  307\n**duc2001**  AP891006-0029  : -->  187  /  307\n**duc2001**  FT933-8941  : -->  188  /  307\n**duc2001**  LA072089-0140  : -->  189  /  307\n**duc2001**  WSJ910107-0139  : -->  190  /  307\n**duc2001**  AP881222-0089  : -->  191  /  307\n**duc2001**  LA102190-0045  : -->  192  /  307\n**duc2001**  FBIS3-22942  : -->  193  /  307\n**duc2001**  FT923-5267  : -->  194  /  307\n**duc2001**  FBIS3-23360  : -->  195  /  307\n**duc2001**  WSJ910405-0154  : -->  196  /  307\n**duc2001**  FT941-4219  : -->  197  /  307\n**duc2001**  WSJ890828-0011  : -->  198  /  307\n**duc2001**  FT923-5797  : -->  199  /  307\n**duc2001**  WSJ911121-0136  : -->  200  /  307\n**duc2001**  SJMN91-06182091  : -->  201  /  307\n**duc2001**  WSJ910304-0002  : -->  202  /  307\n**duc2001**  AP901030-0216  : -->  203  /  307\n**duc2001**  AP900629-0260  : -->  204  /  307\n**duc2001**  FT923-6455  : -->  205  /  307\n**duc2001**  AP880217-0175  : -->  206  /  307\n**duc2001**  LA060490-0083  : -->  207  /  307\n**duc2001**  WSJ870908-0047  : -->  208  /  307\n**duc2001**  FT934-9116  : -->  209  /  307\n**duc2001**  LA010890-0031  : -->  210  /  307\n**duc2001**  FBIS-45908  : -->  211  /  307\n**duc2001**  FT943-12341  : -->  212  /  307\n**duc2001**  LA110490-0184  : -->  213  /  307\n**duc2001**  AP881216-0017  : -->  214  /  307\n**duc2001**  WSJ911031-0012  : -->  215  /  307\n**duc2001**  AP900529-0005  : -->  216  /  307\n**duc2001**  AP900910-0020  : -->  217  /  307\n**duc2001**  SJMN91-06012224  : -->  218  /  307\n**duc2001**  SJMN91-06189077  : -->  219  /  307\n**duc2001**  AP880927-0089  : -->  220  /  307\n**duc2001**  WSJ910208-0130  : -->  221  /  307\n**duc2001**  AP880601-0040  : -->  222  /  307\n**duc2001**  AP900424-0035  : -->  223  /  307\n**duc2001**  FBIS3-41  : -->  224  /  307\n**duc2001**  FBIS4-4674  : -->  225  /  307\n**duc2001**  SJMN91-06184088  : -->  226  /  307\n**duc2001**  AP880903-0092  : -->  227  /  307\n**duc2001**  LA080189-0042  : -->  228  /  307\n**duc2001**  LA050889-0075  : -->  229  /  307\n**duc2001**  LA030789-0047  : -->  230  /  307\n**duc2001**  AP890529-0030  : -->  231  /  307\n**duc2001**  WSJ910326-0090  : -->  232  /  307\n**duc2001**  LA032789-0038  : -->  233  /  307\n**duc2001**  AP880928-0146  : -->  234  /  307\n**duc2001**  AP881206-0114  : -->  235  /  307\n**duc2001**  FT934-5781  : -->  236  /  307\n**duc2001**  AP900306-0105  : -->  237  /  307\n**duc2001**  LA081490-0030  : -->  238  /  307\n**duc2001**  LA070190-0073  : -->  239  /  307\n**duc2001**  AP880520-0264  : -->  240  /  307\n**duc2001**  AP890501-0176  : -->  241  /  307\n**duc2001**  AP900322-0200  : -->  242  /  307\n**duc2001**  FT932-5855  : -->  243  /  307\n**duc2001**  SJMN91-06161012  : -->  244  /  307\n**duc2001**  AP880801-0195  : -->  245  /  307\n**duc2001**  AP900215-0031  : -->  246  /  307\n**duc2001**  AP890228-0019  : -->  247  /  307\n**duc2001**  AP890227-0016  : -->  248  /  307\n**duc2001**  FBIS3-30788  : -->  249  /  307\n**duc2001**  FT941-1750  : -->  250  /  307\n**duc2001**  LA093089-0076  : -->  251  /  307\n**duc2001**  FT933-8272  : -->  252  /  307\n**duc2001**  LA092490-0095  : -->  253  /  307\n**duc2001**  AP880927-0117  : -->  254  /  307\n**duc2001**  AP890326-0081  : -->  255  /  307\n**duc2001**  AP890922-0167  : -->  256  /  307\n**duc2001**  AP900428-0005  : -->  257  /  307\n**duc2001**  AP890316-0018  : -->  258  /  307\n**duc2001**  SJMN91-06191081  : -->  259  /  307\n**duc2001**  SJMN91-06193081  : -->  260  /  307\n**duc2001**  LA040689-0056  : -->  261  /  307\n**duc2001**  AP880811-0299  : -->  262  /  307\n**duc2001**  LA093089-0126  : -->  263  /  307\n**duc2001**  LA102189-0151  : -->  264  /  307\n**duc2001**  WSJ880923-0163  : -->  265  /  307\n**duc2001**  FBIS4-67721  : -->  266  /  307\n**duc2001**  AP890801-0025  : -->  267  /  307\n**duc2001**  LA101289-0194  : -->  268  /  307\n**duc2001**  FT931-3883  : -->  269  /  307\n**duc2001**  AP880705-0018  : -->  270  /  307\n**duc2001**  LA080790-0111  : -->  271  /  307\n**duc2001**  WSJ900418-0193  : -->  272  /  307\n**duc2001**  LA042490-0142  : -->  273  /  307\n**duc2001**  SJMN91-06255434  : -->  274  /  307\n**duc2001**  AP880330-0119  : -->  275  /  307\n**duc2001**  AP890719-0225  : -->  276  /  307\n**duc2001**  AP900323-0036  : -->  277  /  307\n**duc2001**  AP890302-0063  : -->  278  /  307\n**duc2001**  AP900625-0160  : -->  279  /  307\n**duc2001**  SJMN91-06301029  : -->  280  /  307\n**duc2001**  FT923-5859  : -->  281  /  307\n**duc2001**  AP881211-0027  : -->  282  /  307\n**duc2001**  AP900322-0192  : -->  283  /  307\n**duc2001**  LA101690-0040  : -->  284  /  307\n**duc2001**  WSJ910304-0005  : -->  285  /  307\n**duc2001**  AP900511-0159  : -->  286  /  307\n**duc2001**  LA021689-0227  : -->  287  /  307\n**duc2001**  AP890722-0081  : -->  288  /  307\n**duc2001**  SJMN91-06276078  : -->  289  /  307\n**duc2001**  WSJ920114-0145  : -->  290  /  307\n**duc2001**  SJMN91-06283083  : -->  291  /  307\n**duc2001**  FT934-13350  : -->  292  /  307\n**duc2001**  LA110589-0082  : -->  293  /  307\n**duc2001**  FT933-6011  : -->  294  /  307\n**duc2001**  LA103089-0043  : -->  295  /  307\n**duc2001**  SJMN91-06187248  : -->  296  /  307\n**duc2001**  FT923-7126  : -->  297  /  307\n**duc2001**  SJMN91-06071022  : -->  298  /  307\n**duc2001**  LA070189-0080  : -->  299  /  307\n**duc2001**  WSJ910607-0063  : -->  300  /  307\n**duc2001**  WSJ871216-0037  : -->  301  /  307\n**duc2001**  AP881227-0185  : -->  302  /  307\n**duc2001**  AP880318-0051  : -->  303  /  307\n**duc2001**  LA101090-0017  : -->  304  /  307\n**duc2001**  SJMN91-06136305  : -->  305  /  307\n**duc2001**  SJMN91-06195131  : -->  306  /  307\nDone\n","output_type":"stream"}],"execution_count":4}]}